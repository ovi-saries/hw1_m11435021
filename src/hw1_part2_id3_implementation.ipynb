{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863c705b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ID3 Decision Tree Implementation\n",
      "Includes: Discretization, Post-Pruning, Test Set Evaluation\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Starting ID3 Decision Tree Training\n",
      "============================================================\n",
      "============================================================\n",
      "Loading raw data\n",
      "============================================================\n",
      "Training data shape: (32561, 15)\n",
      "Test data shape: (16281, 15)\n",
      "\n",
      "Cleaning data...\n",
      "Training data: 32561 → 30162 (removed 2399 rows)\n",
      "Test data: 16281 → 15060 (removed 1221 rows)\n",
      "\n",
      "Encoding categorical features...\n",
      "✓ Encoding complete for 8 categorical features\n",
      "\n",
      "Discretizing continuous features (n_bins=10)...\n",
      "✓ Discretization complete, each feature divided into 10 bins\n",
      "\n",
      "============================================================\n",
      "Data preparation complete\n",
      "============================================================\n",
      "Number of features: 14\n",
      "  - Continuous features: 6\n",
      "  - Categorical features: 8\n",
      "Discretized: True\n",
      "\n",
      "Data split:\n",
      "  Training set: (24129, 14)\n",
      "  Validation set: (6033, 14)\n",
      "  Test set: (15060, 14)\n",
      "\n",
      "Label distribution (>50K proportion):\n",
      "  Training set: 24.89%\n",
      "  Validation set: 24.90%\n",
      "  Test set: 24.57%\n",
      "============================================================\n",
      "\n",
      "\n",
      "Training ID3 model (max_depth=10, min_samples_split=20)...\n",
      "\n",
      "Tree Statistics:\n",
      "  Number of nodes: 4065\n",
      "  Tree depth: 8\n",
      "\n",
      "Before Pruning:\n",
      "  Training accuracy: 0.8719\n",
      "  Validation accuracy: 0.8145\n",
      "\n",
      "Starting post-pruning...\n",
      "Validation accuracy before pruning: 0.8145\n",
      "Validation accuracy after pruning: 0.8440\n",
      "Accuracy change: +0.0295\n",
      "\n",
      "Tree Statistics After Pruning:\n",
      "  Number of nodes: 895 (reduced by 3170)\n",
      "  Tree depth: 8\n",
      "\n",
      "After Pruning:\n",
      "  Training accuracy: 0.8452 (-0.0267)\n",
      "  Validation accuracy: 0.8440 (+0.0295)\n",
      "\n",
      "============================================================\n",
      "Test Set Results:\n",
      "============================================================\n",
      "Test accuracy: 0.8231\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.86      0.91      0.89     11360\n",
      "        >50K       0.67      0.55      0.61      3700\n",
      "\n",
      "    accuracy                           0.82     15060\n",
      "   macro avg       0.77      0.73      0.75     15060\n",
      "weighted avg       0.81      0.82      0.82     15060\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[10345  1015]\n",
      " [ 1649  2051]]\n",
      "  True Negatives:  10345\n",
      "  False Positives: 1015\n",
      "  False Negatives: 1649\n",
      "  True Positives:  2051\n",
      "\n",
      "============================================================\n",
      "Decision Tree Structure Preview (Top 3 Levels):\n",
      "============================================================\n",
      "Feature 10: relationship (samples: 24129)\n",
      "  0 => Feature 7: education (samples: 10011)\n",
      "    0 => Class: 0 (samples: 242)\n",
      "    1 => Class: 0 (samples: 231)\n",
      "    10 => Class: 1 (samples: 198)\n",
      "    11 => Feature 0: age (samples: 3232)\n",
      "      0 => Class: 0 (samples: 27)\n",
      "      1 => Class: 0 (samples: 124)\n",
      "      2 => Class: 0 (samples: 299)\n",
      "      3 => Class: 0 (samples: 270)\n",
      "      4 => Class: 0 (samples: 421)\n",
      "      5 => Feature 9: occupation (samples: 381)\n",
      "        ... (depth limit reached)\n",
      "      6 => Feature 13: native_country (samples: 352)\n",
      "        ... (depth limit reached)\n",
      "      7 => Feature 5: hours_per_week (samples: 401)\n",
      "        ... (depth limit reached)\n",
      "      8 => Feature 9: occupation (samples: 503)\n",
      "        ... (depth limit reached)\n",
      "      9 => Feature 9: occupation (samples: 454)\n",
      "        ... (depth limit reached)\n",
      "    12 => Feature 9: occupation (samples: 647)\n",
      "      0 => Class: 1 (samples: 23)\n",
      "      1 => Class: 1 (samples: 1)\n",
      "      10 => Class: 1 (samples: 6)\n",
      "      11 => Feature 5: hours_per_week (samples: 59)\n",
      "        ... (depth limit reached)\n",
      "      12 => Class: 1 (samples: 14)\n",
      "      13 => Class: 0 (samples: 3)\n",
      "      2 => Class: 1 (samples: 11)\n",
      "      3 => Feature 5: hours_per_week (samples: 238)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 4)\n",
      "      5 => Class: 0 (samples: 2)\n",
      "      6 => Class: 0 (samples: 3)\n",
      "      7 => Class: 0 (samples: 4)\n",
      "      9 => Feature 0: age (samples: 279)\n",
      "        ... (depth limit reached)\n",
      "    13 => Class: 0 (samples: 10)\n",
      "    14 => Class: 1 (samples: 286)\n",
      "    15 => Feature 9: occupation (samples: 1917)\n",
      "      0 => Class: 0 (samples: 118)\n",
      "      10 => Feature 0: age (samples: 91)\n",
      "        ... (depth limit reached)\n",
      "      11 => Feature 0: age (samples: 284)\n",
      "        ... (depth limit reached)\n",
      "      12 => Feature 0: age (samples: 70)\n",
      "        ... (depth limit reached)\n",
      "      13 => Feature 6: workclass (samples: 139)\n",
      "        ... (depth limit reached)\n",
      "      2 => Feature 0: age (samples: 424)\n",
      "        ... (depth limit reached)\n",
      "      3 => Feature 6: workclass (samples: 332)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 81)\n",
      "      5 => Class: 0 (samples: 66)\n",
      "      6 => Class: 0 (samples: 114)\n",
      "      7 => Class: 0 (samples: 71)\n",
      "      9 => Feature 0: age (samples: 127)\n",
      "        ... (depth limit reached)\n",
      "    2 => Class: 0 (samples: 70)\n",
      "    3 => Class: 0 (samples: 50)\n",
      "    4 => Class: 0 (samples: 104)\n",
      "    5 => Class: 0 (samples: 236)\n",
      "    6 => Class: 0 (samples: 147)\n",
      "    7 => Feature 9: occupation (samples: 306)\n",
      "      0 => Class: 0 (samples: 30)\n",
      "      10 => Class: 0 (samples: 22)\n",
      "      11 => Class: 1 (samples: 48)\n",
      "      12 => Class: 1 (samples: 25)\n",
      "      13 => Class: 0 (samples: 11)\n",
      "      2 => Class: 0 (samples: 50)\n",
      "      3 => Feature 0: age (samples: 46)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 7)\n",
      "      5 => Class: 0 (samples: 8)\n",
      "      6 => Class: 0 (samples: 14)\n",
      "      7 => Class: 0 (samples: 9)\n",
      "      9 => Feature 1: fnlwgt (samples: 36)\n",
      "        ... (depth limit reached)\n",
      "    8 => Feature 0: age (samples: 477)\n",
      "      0 => Class: 0 (samples: 2)\n",
      "      1 => Class: 0 (samples: 17)\n",
      "      2 => Feature 1: fnlwgt (samples: 39)\n",
      "        ... (depth limit reached)\n",
      "      3 => Class: 0 (samples: 46)\n",
      "      4 => Feature 9: occupation (samples: 61)\n",
      "        ... (depth limit reached)\n",
      "      5 => Feature 9: occupation (samples: 86)\n",
      "        ... (depth limit reached)\n",
      "      6 => Class: 0 (samples: 60)\n",
      "      7 => Class: 1 (samples: 68)\n",
      "      8 => Class: 1 (samples: 64)\n",
      "      9 => Feature 9: occupation (samples: 34)\n",
      "        ... (depth limit reached)\n",
      "    9 => Feature 9: occupation (samples: 1858)\n",
      "      0 => Class: 1 (samples: 99)\n",
      "      10 => Class: 1 (samples: 53)\n",
      "      11 => Class: 1 (samples: 328)\n",
      "      12 => Class: 1 (samples: 62)\n",
      "      13 => Feature 0: age (samples: 28)\n",
      "        ... (depth limit reached)\n",
      "      2 => Class: 1 (samples: 113)\n",
      "      3 => Class: 1 (samples: 607)\n",
      "      4 => Feature 1: fnlwgt (samples: 38)\n",
      "        ... (depth limit reached)\n",
      "      5 => Class: 0 (samples: 12)\n",
      "      6 => Feature 1: fnlwgt (samples: 30)\n",
      "        ... (depth limit reached)\n",
      "      7 => Feature 5: hours_per_week (samples: 31)\n",
      "        ... (depth limit reached)\n",
      "      9 => Feature 5: hours_per_week (samples: 457)\n",
      "        ... (depth limit reached)\n",
      "  1 => Feature 7: education (samples: 6241)\n",
      "    0 => Class: 0 (samples: 144)\n",
      "    1 => Class: 0 (samples: 155)\n",
      "    10 => Class: 1 (samples: 70)\n",
      "    11 => Class: 0 (samples: 1874)\n",
      "    12 => Feature 5: hours_per_week (samples: 409)\n",
      "      0 => Class: 0 (samples: 28)\n",
      "      1 => Class: 0 (samples: 53)\n",
      "      2 => Class: 0 (samples: 12)\n",
      "      3 => Feature 8: marital_status (samples: 151)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 48)\n",
      "      5 => Feature 9: occupation (samples: 55)\n",
      "        ... (depth limit reached)\n",
      "      6 => Class: 0 (samples: 62)\n",
      "    13 => Class: 0 (samples: 18)\n",
      "    14 => Feature 0: age (samples: 98)\n",
      "      1 => Class: 0 (samples: 2)\n",
      "      2 => Class: 0 (samples: 18)\n",
      "      3 => Class: 0 (samples: 12)\n",
      "      4 => Class: 1 (samples: 9)\n",
      "      5 => Class: 1 (samples: 13)\n",
      "      6 => Class: 1 (samples: 9)\n",
      "      7 => Class: 1 (samples: 14)\n",
      "      8 => Class: 1 (samples: 13)\n",
      "      9 => Class: 0 (samples: 8)\n",
      "    15 => Feature 0: age (samples: 1364)\n",
      "      0 => Class: 0 (samples: 139)\n",
      "      1 => Class: 0 (samples: 208)\n",
      "      2 => Class: 0 (samples: 177)\n",
      "      3 => Class: 0 (samples: 108)\n",
      "      4 => Class: 0 (samples: 126)\n",
      "      5 => Class: 0 (samples: 120)\n",
      "      6 => Class: 0 (samples: 127)\n",
      "      7 => Class: 0 (samples: 116)\n",
      "      8 => Class: 0 (samples: 116)\n",
      "      9 => Feature 5: hours_per_week (samples: 127)\n",
      "        ... (depth limit reached)\n",
      "    2 => Class: 0 (samples: 61)\n",
      "    3 => Class: 0 (samples: 40)\n",
      "    4 => Class: 0 (samples: 47)\n",
      "    5 => Class: 0 (samples: 87)\n",
      "    6 => Class: 0 (samples: 75)\n",
      "    7 => Class: 0 (samples: 253)\n",
      "    8 => Class: 0 (samples: 285)\n",
      "    9 => Feature 0: age (samples: 1261)\n",
      "      0 => Class: 0 (samples: 2)\n",
      "      1 => Class: 0 (samples: 224)\n",
      "      2 => Feature 5: hours_per_week (samples: 244)\n",
      "        ... (depth limit reached)\n",
      "      3 => Class: 0 (samples: 154)\n",
      "      4 => Feature 5: hours_per_week (samples: 160)\n",
      "        ... (depth limit reached)\n",
      "      5 => Feature 9: occupation (samples: 128)\n",
      "        ... (depth limit reached)\n",
      "      6 => Class: 0 (samples: 90)\n",
      "      7 => Class: 0 (samples: 105)\n",
      "      8 => Class: 0 (samples: 79)\n",
      "      9 => Class: 0 (samples: 75)\n",
      "  2 => Class: 0 (samples: 718)\n",
      "  3 => Class: 0 (samples: 3504)\n",
      "  4 => Feature 7: education (samples: 2551)\n",
      "    0 => Class: 0 (samples: 84)\n",
      "    1 => Class: 0 (samples: 103)\n",
      "    10 => Class: 1 (samples: 20)\n",
      "    11 => Class: 0 (samples: 970)\n",
      "    12 => Class: 0 (samples: 105)\n",
      "    13 => Class: 0 (samples: 1)\n",
      "    14 => Class: 0 (samples: 17)\n",
      "    15 => Class: 0 (samples: 584)\n",
      "    2 => Class: 0 (samples: 26)\n",
      "    3 => Class: 0 (samples: 14)\n",
      "    4 => Class: 0 (samples: 29)\n",
      "    5 => Class: 0 (samples: 53)\n",
      "    6 => Class: 0 (samples: 50)\n",
      "    7 => Class: 0 (samples: 99)\n",
      "    8 => Class: 0 (samples: 145)\n",
      "    9 => Class: 0 (samples: 251)\n",
      "  5 => Feature 7: education (samples: 1104)\n",
      "    0 => Class: 0 (samples: 17)\n",
      "    1 => Class: 0 (samples: 20)\n",
      "    10 => Class: 1 (samples: 16)\n",
      "    11 => Feature 9: occupation (samples: 359)\n",
      "      0 => Feature 6: workclass (samples: 118)\n",
      "        ... (depth limit reached)\n",
      "      10 => Class: 0 (samples: 2)\n",
      "      11 => Class: 0 (samples: 45)\n",
      "      12 => Class: 0 (samples: 9)\n",
      "      13 => Class: 0 (samples: 7)\n",
      "      2 => Class: 1 (samples: 9)\n",
      "      3 => Feature 1: fnlwgt (samples: 43)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 6)\n",
      "      5 => Class: 0 (samples: 7)\n",
      "      6 => Class: 0 (samples: 34)\n",
      "      7 => Class: 0 (samples: 60)\n",
      "      8 => Class: 0 (samples: 4)\n",
      "      9 => Class: 0 (samples: 15)\n",
      "    12 => Feature 1: fnlwgt (samples: 81)\n",
      "      0 => Class: 1 (samples: 10)\n",
      "      1 => Class: 1 (samples: 11)\n",
      "      2 => Class: 1 (samples: 5)\n",
      "      3 => Class: 1 (samples: 7)\n",
      "      4 => Class: 1 (samples: 11)\n",
      "      5 => Class: 1 (samples: 12)\n",
      "      6 => Class: 1 (samples: 12)\n",
      "      7 => Class: 0 (samples: 2)\n",
      "      8 => Class: 1 (samples: 3)\n",
      "      9 => Class: 1 (samples: 8)\n",
      "    13 => Class: 0 (samples: 2)\n",
      "    14 => Class: 1 (samples: 19)\n",
      "    15 => Feature 9: occupation (samples: 218)\n",
      "      0 => Feature 1: fnlwgt (samples: 78)\n",
      "        ... (depth limit reached)\n",
      "      10 => Class: 1 (samples: 1)\n",
      "      11 => Class: 0 (samples: 26)\n",
      "      12 => Class: 0 (samples: 9)\n",
      "      13 => Class: 0 (samples: 3)\n",
      "      2 => Class: 0 (samples: 3)\n",
      "      3 => Feature 0: age (samples: 40)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 2)\n",
      "      5 => Class: 0 (samples: 2)\n",
      "      6 => Class: 0 (samples: 4)\n",
      "      7 => Class: 0 (samples: 26)\n",
      "      9 => Feature 1: fnlwgt (samples: 24)\n",
      "        ... (depth limit reached)\n",
      "    2 => Class: 0 (samples: 12)\n",
      "    3 => Class: 0 (samples: 1)\n",
      "    4 => Class: 0 (samples: 5)\n",
      "    5 => Class: 0 (samples: 17)\n",
      "    6 => Class: 0 (samples: 15)\n",
      "    7 => Feature 0: age (samples: 54)\n",
      "      0 => Class: 1 (samples: 1)\n",
      "      1 => Class: 0 (samples: 6)\n",
      "      2 => Class: 0 (samples: 8)\n",
      "      3 => Class: 1 (samples: 6)\n",
      "      4 => Class: 1 (samples: 11)\n",
      "      5 => Class: 1 (samples: 10)\n",
      "      6 => Class: 0 (samples: 5)\n",
      "      7 => Class: 1 (samples: 3)\n",
      "      8 => Class: 0 (samples: 3)\n",
      "      9 => Class: 1 (samples: 1)\n",
      "    8 => Feature 9: occupation (samples: 55)\n",
      "      0 => Class: 0 (samples: 13)\n",
      "      11 => Class: 1 (samples: 1)\n",
      "      12 => Class: 1 (samples: 8)\n",
      "      13 => Class: 1 (samples: 1)\n",
      "      2 => Class: 0 (samples: 1)\n",
      "      3 => Class: 1 (samples: 7)\n",
      "      5 => Class: 0 (samples: 1)\n",
      "      6 => Class: 0 (samples: 1)\n",
      "      7 => Class: 0 (samples: 6)\n",
      "      9 => Class: 1 (samples: 16)\n",
      "    9 => Feature 13: native_country (samples: 213)\n",
      "      10 => Class: 1 (samples: 3)\n",
      "      13 => Class: 1 (samples: 1)\n",
      "      19 => Class: 1 (samples: 1)\n",
      "      23 => Class: 1 (samples: 1)\n",
      "      25 => Class: 0 (samples: 1)\n",
      "      29 => Class: 1 (samples: 6)\n",
      "      30 => Class: 0 (samples: 1)\n",
      "      32 => Class: 0 (samples: 1)\n",
      "      34 => Class: 0 (samples: 3)\n",
      "      35 => Class: 0 (samples: 2)\n",
      "      36 => Class: 0 (samples: 1)\n",
      "      38 => Feature 0: age (samples: 189)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 2)\n",
      "      8 => Class: 1 (samples: 1)\n",
      "\n",
      "============================================================\n",
      "Training Complete! Final Results Summary\n",
      "============================================================\n",
      "Training accuracy: 0.8452\n",
      "Validation accuracy: 0.8440\n",
      "Test accuracy: 0.8231\n",
      "Decision tree nodes: 895\n",
      "Decision tree depth: 8\n",
      "\n",
      "============================================================\n",
      "To compare with no pruning, run:\n",
      "train_and_evaluate_id3(use_pruning=False)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from hw1_part2_preprocessing import UnifiedDataPreprocessor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class ID3DecisionTree:\n",
    "    \"\"\"Decision tree classifier using the ID3 algorithm with optional pruning.\n",
    "\n",
    "    Attributes:\n",
    "        max_depth (int, optional): Maximum depth of the tree. Defaults to None.\n",
    "        min_samples_split (int): Minimum number of samples required to split. Defaults to 2.\n",
    "        tree (dict): The constructed decision tree.\n",
    "        feature_names (list): Names of features used in the tree.\n",
    "        majority_class (int): Majority class for handling unknown values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        \"\"\"Initialize the ID3 decision tree.\n",
    "\n",
    "        Args:\n",
    "            max_depth (int, optional): Maximum depth of the tree. Defaults to None.\n",
    "            min_samples_split (int): Minimum number of samples required to split. Defaults to 2.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "        self.feature_names = None\n",
    "        self.majority_class = None\n",
    "\n",
    "    def compute_entropy(self, labels):\n",
    "        \"\"\"Calculate the entropy of a label set.\n",
    "\n",
    "        Args:\n",
    "            labels (np.ndarray): Array of class labels.\n",
    "\n",
    "        Returns:\n",
    "            float: Entropy value of the label set.\n",
    "        \"\"\"\n",
    "        if len(labels) == 0:\n",
    "            return 0\n",
    "        _, counts = np.unique(labels, return_counts=True)\n",
    "        probabilities = counts / len(labels)\n",
    "        return -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "\n",
    "    def compute_information_gain(self, features, labels, feature_idx):\n",
    "        \"\"\"Calculate the information gain for a specific feature.\n",
    "\n",
    "        Args:\n",
    "            features (np.ndarray): Feature matrix.\n",
    "            labels (np.ndarray): Array of class labels.\n",
    "            feature_idx (int): Index of the feature to evaluate.\n",
    "\n",
    "        Returns:\n",
    "            float: Information gain for the specified feature.\n",
    "        \"\"\"\n",
    "        parent_entropy = self.compute_entropy(labels)\n",
    "        values, counts = np.unique(features[:, feature_idx], return_counts=True)\n",
    "        weighted_entropy = 0\n",
    "\n",
    "        total_samples = len(labels)\n",
    "        for value, count in zip(values, counts):\n",
    "            mask = features[:, feature_idx] == value\n",
    "            child_samples = len(labels[mask])\n",
    "            if child_samples > 0:\n",
    "                child_entropy = self.compute_entropy(labels[mask])\n",
    "                weighted_entropy += (child_samples / total_samples) * child_entropy\n",
    "\n",
    "        return parent_entropy - weighted_entropy\n",
    "\n",
    "    def build_tree(self, features, labels, depth=0, features_to_consider=None):\n",
    "        \"\"\"Recursively build the decision tree.\n",
    "\n",
    "        Args:\n",
    "            features (np.ndarray): Feature matrix.\n",
    "            labels (np.ndarray): Array of class labels.\n",
    "            depth (int): Current depth of the tree. Defaults to 0.\n",
    "            features_to_consider (list, optional): List of feature indices to consider. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            dict: Node of the decision tree.\n",
    "        \"\"\"\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(unique_labels) == 1:\n",
    "            return {\"class\": int(unique_labels[0]), \"samples\": len(labels)}\n",
    "\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            majority_class = int(np.argmax(np.bincount(labels)))\n",
    "            return {\"class\": majority_class, \"samples\": len(labels)}\n",
    "\n",
    "        if len(labels) < self.min_samples_split:\n",
    "            majority_class = int(np.argmax(np.bincount(labels)))\n",
    "            return {\"class\": majority_class, \"samples\": len(labels)}\n",
    "\n",
    "        if features_to_consider is None:\n",
    "            features_to_consider = list(range(features.shape[1]))\n",
    "\n",
    "        if len(features_to_consider) == 0:\n",
    "            majority_class = int(np.argmax(np.bincount(labels)))\n",
    "            return {\"class\": majority_class, \"samples\": len(labels)}\n",
    "\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        for feature in features_to_consider:\n",
    "            gain = self.compute_information_gain(features, labels, feature)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "\n",
    "        if best_feature is None or best_gain < 1e-5:\n",
    "            majority_class = int(np.argmax(np.bincount(labels)))\n",
    "            return {\"class\": majority_class, \"samples\": len(labels)}\n",
    "\n",
    "        children = {}\n",
    "        feature_values = np.unique(features[:, best_feature])\n",
    "        for value in feature_values:\n",
    "            mask = features[:, best_feature] == value\n",
    "            child_features, child_labels = features[mask], labels[mask]\n",
    "            if len(child_labels) == 0:\n",
    "                majority_class = int(np.argmax(np.bincount(labels)))\n",
    "                children[value] = {\"class\": majority_class, \"samples\": 0}\n",
    "            else:\n",
    "                new_features = [f for f in features_to_consider if f != best_feature]\n",
    "                children[value] = self.build_tree(child_features, child_labels, depth + 1, new_features)\n",
    "\n",
    "        return {\n",
    "            \"feature\": best_feature,\n",
    "            \"children\": children,\n",
    "            \"samples\": len(labels),\n",
    "            \"class\": int(np.argmax(np.bincount(labels))),\n",
    "        }\n",
    "\n",
    "    def fit(self, features, labels):\n",
    "        \"\"\"Train the decision tree model.\n",
    "\n",
    "        Args:\n",
    "            features (np.ndarray): Training feature matrix.\n",
    "            labels (np.ndarray): Training class labels.\n",
    "\n",
    "        Returns:\n",
    "            ID3DecisionTree: Trained model instance.\n",
    "        \"\"\"\n",
    "        self.majority_class = int(np.argmax(np.bincount(labels)))\n",
    "        self.tree = self.build_tree(features, labels)\n",
    "        if self.feature_names is None:\n",
    "            self.feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n",
    "        return self\n",
    "\n",
    "    def predict_one(self, sample, node):\n",
    "        \"\"\"Predict the class for a single sample.\n",
    "\n",
    "        Args:\n",
    "            sample (np.ndarray): Single feature vector.\n",
    "            node (dict): Current node in the decision tree.\n",
    "\n",
    "        Returns:\n",
    "            int: Predicted class label.\n",
    "        \"\"\"\n",
    "        if \"class\" in node and \"children\" not in node:\n",
    "            return node[\"class\"]\n",
    "\n",
    "        if \"children\" in node:\n",
    "            feature_idx = node[\"feature\"]\n",
    "            feature_value = sample[feature_idx]\n",
    "            if feature_value in node[\"children\"]:\n",
    "                return self.predict_one(sample, node[\"children\"][feature_value])\n",
    "            return node.get(\"class\", self.majority_class)\n",
    "\n",
    "        return node.get(\"class\", self.majority_class)\n",
    "\n",
    "    def predict(self, features):\n",
    "        \"\"\"Predict classes for a dataset.\n",
    "\n",
    "        Args:\n",
    "            features (np.ndarray): Feature matrix to predict.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Predicted class labels.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the model is not fitted.\n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            raise ValueError(\"Model must be fitted before prediction\")\n",
    "\n",
    "        predictions = []\n",
    "        for i in range(features.shape[0]):\n",
    "            try:\n",
    "                pred = self.predict_one(features[i], self.tree)\n",
    "                predictions.append(pred)\n",
    "            except Exception as e:\n",
    "                print(f\"Prediction error for sample {i}: {e}\")\n",
    "                predictions.append(self.majority_class)\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def prune(self, validation_features, validation_labels):\n",
    "        \"\"\"Apply post-pruning to the decision tree using validation data.\n",
    "\n",
    "        Args:\n",
    "            validation_features (np.ndarray): Validation feature matrix.\n",
    "            validation_labels (np.ndarray): Validation class labels.\n",
    "\n",
    "        Returns:\n",
    "            ID3DecisionTree: Pruned model instance.\n",
    "        \"\"\"\n",
    "        print(\"\\nStarting post-pruning...\")\n",
    "        initial_acc = accuracy_score(validation_labels, self.predict(validation_features))\n",
    "        print(f\"Validation accuracy before pruning: {initial_acc:.4f}\")\n",
    "\n",
    "        self.tree = self._prune_node(self.tree, validation_features, validation_labels)\n",
    "\n",
    "        final_acc = accuracy_score(validation_labels, self.predict(validation_features))\n",
    "        print(f\"Validation accuracy after pruning: {final_acc:.4f}\")\n",
    "        print(f\"Accuracy change: {final_acc - initial_acc:+.4f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _prune_node(self, node, validation_features, validation_labels):\n",
    "        \"\"\"Recursively prune the decision tree.\n",
    "\n",
    "        Args:\n",
    "            node (dict): Current node in the decision tree.\n",
    "            validation_features (np.ndarray): Validation feature matrix.\n",
    "            validation_labels (np.ndarray): Validation class labels.\n",
    "\n",
    "        Returns:\n",
    "            dict: Pruned node.\n",
    "        \"\"\"\n",
    "        if \"children\" not in node:\n",
    "            return node\n",
    "\n",
    "        for value in list(node[\"children\"].keys()):\n",
    "            node[\"children\"][value] = self._prune_node(\n",
    "                node[\"children\"][value], validation_features, validation_labels\n",
    "            )\n",
    "\n",
    "        pred_before = self.predict(validation_features)\n",
    "        error_before = np.mean(pred_before != validation_labels)\n",
    "\n",
    "        original_children = node[\"children\"]\n",
    "        node[\"children\"] = {}\n",
    "        pred_after = self.predict(validation_features)\n",
    "        error_after = np.mean(pred_after != validation_labels)\n",
    "\n",
    "        if error_after <= error_before:\n",
    "            del node[\"children\"]\n",
    "            del node[\"feature\"]\n",
    "            return node\n",
    "\n",
    "        node[\"children\"] = original_children\n",
    "        return node\n",
    "\n",
    "    def count_nodes(self):\n",
    "        \"\"\"Count the total number of nodes in the tree.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of nodes in the tree.\n",
    "        \"\"\"\n",
    "        def count_recursive(node):\n",
    "            if \"children\" not in node:\n",
    "                return 1\n",
    "            return 1 + sum(count_recursive(child) for child in node[\"children\"].values())\n",
    "\n",
    "        return count_recursive(self.tree) if self.tree else 0\n",
    "\n",
    "    def get_depth(self):\n",
    "        \"\"\"Calculate the depth of the tree.\n",
    "\n",
    "        Returns:\n",
    "            int: Depth of the decision tree.\n",
    "        \"\"\"\n",
    "        def depth_recursive(node):\n",
    "            if \"children\" not in node:\n",
    "                return 1\n",
    "            if not node[\"children\"]:\n",
    "                return 1\n",
    "            return 1 + max(depth_recursive(child) for child in node[\"children\"].values())\n",
    "\n",
    "        return depth_recursive(self.tree) if self.tree else 0\n",
    "\n",
    "    def print_tree(self, max_depth=None):\n",
    "        \"\"\"Print the structure of the decision tree.\n",
    "\n",
    "        Args:\n",
    "            max_depth (int, optional): Maximum depth to print. Defaults to None.\n",
    "        \"\"\"\n",
    "        def print_recursive(node, depth=0, prefix=\"\"):\n",
    "            indent = \"  \" * depth\n",
    "            if \"class\" in node and \"children\" not in node:\n",
    "                print(f\"{indent}{prefix}Class: {node['class']} (samples: {node['samples']})\")\n",
    "                return\n",
    "\n",
    "            feature_name = (\n",
    "                self.feature_names[node[\"feature\"]]\n",
    "                if self.feature_names\n",
    "                else node[\"feature\"]\n",
    "            )\n",
    "            print(f\"{indent}{prefix}Feature {feature_name} (samples: {node['samples']})\")\n",
    "\n",
    "            if max_depth is not None and depth >= max_depth:\n",
    "                print(f\"{indent}  ... (depth limit reached)\")\n",
    "                return\n",
    "\n",
    "            for value, child in sorted(node[\"children\"].items(), key=lambda x: str(x[0])):\n",
    "                print_recursive(child, depth + 1, f\"{value} => \")\n",
    "\n",
    "        print_recursive(self.tree)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"Save the model to a file.\n",
    "\n",
    "        Args:\n",
    "            path (str or Path): Path to save the model.\n",
    "        \"\"\"\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(path):\n",
    "        \"\"\"Load a model from a file.\n",
    "\n",
    "        Args:\n",
    "            path (str or Path): Path to the model file.\n",
    "\n",
    "        Returns:\n",
    "            ID3DecisionTree: Loaded model instance.\n",
    "        \"\"\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "\n",
    "def train_and_evaluate_id3(use_pruning=True):\n",
    "    \"\"\"Train and evaluate the ID3 decision tree model.\n",
    "\n",
    "    Args:\n",
    "        use_pruning (bool): Whether to apply post-pruning. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing model, accuracies, and tree statistics.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If required data files are missing.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Starting ID3 Decision Tree Training\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        preprocessor = UnifiedDataPreprocessor()\n",
    "        train_features, val_features, test_features, train_labels, val_labels, test_labels = (\n",
    "            preprocessor.get_processed_data(\n",
    "                discretize=True, n_bins=10, validation_split=0.2, random_state=42\n",
    "            )\n",
    "        )\n",
    "        feature_names = preprocessor.get_feature_names()\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Ensure the 'data' directory exists with adult.data and adult.test files\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nTraining ID3 model (max_depth=10, min_samples_split=20)...\")\n",
    "    id3_model = ID3DecisionTree(max_depth=10, min_samples_split=20)\n",
    "    id3_model.feature_names = [f\"{i}: {name}\" for i, name in enumerate(feature_names)]\n",
    "    id3_model.fit(train_features, train_labels)\n",
    "\n",
    "    num_nodes = id3_model.count_nodes()\n",
    "    tree_depth = id3_model.get_depth()\n",
    "    print(f\"\\nTree Statistics:\")\n",
    "    print(f\"  Number of nodes: {num_nodes}\")\n",
    "    print(f\"  Tree depth: {tree_depth}\")\n",
    "\n",
    "    train_pred = id3_model.predict(train_features)\n",
    "    val_pred = id3_model.predict(val_features)\n",
    "    train_acc_before = accuracy_score(train_labels, train_pred)\n",
    "    val_acc_before = accuracy_score(val_labels, val_pred)\n",
    "\n",
    "    print(f\"\\nBefore Pruning:\")\n",
    "    print(f\"  Training accuracy: {train_acc_before:.4f}\")\n",
    "    print(f\"  Validation accuracy: {val_acc_before:.4f}\")\n",
    "\n",
    "    if use_pruning:\n",
    "        id3_model.prune(val_features, val_labels)\n",
    "        num_nodes_after = id3_model.count_nodes()\n",
    "        tree_depth_after = id3_model.get_depth()\n",
    "        print(f\"\\nTree Statistics After Pruning:\")\n",
    "        print(f\"  Number of nodes: {num_nodes_after} (reduced by {num_nodes - num_nodes_after})\")\n",
    "        print(f\"  Tree depth: {tree_depth_after}\")\n",
    "\n",
    "        train_pred = id3_model.predict(train_features)\n",
    "        val_pred = id3_model.predict(val_features)\n",
    "        train_acc_after = accuracy_score(train_labels, train_pred)\n",
    "        val_acc_after = accuracy_score(val_labels, val_pred)\n",
    "\n",
    "        print(f\"\\nAfter Pruning:\")\n",
    "        print(f\"  Training accuracy: {train_acc_after:.4f} ({train_acc_after - train_acc_before:+.4f})\")\n",
    "        print(f\"  Validation accuracy: {val_acc_after:.4f} ({val_acc_after - val_acc_before:+.4f})\")\n",
    "    else:\n",
    "        train_acc_after, val_acc_after = train_acc_before, val_acc_before\n",
    "        num_nodes_after, tree_depth_after = num_nodes, tree_depth\n",
    "\n",
    "    if test_features is not None and test_labels is not None:\n",
    "        test_pred = id3_model.predict(test_features)\n",
    "        test_acc = accuracy_score(test_labels, test_pred)\n",
    "\n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"Test Set Results:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "        print(\"\\nClassification Report (Test Set):\")\n",
    "        print(classification_report(test_labels, test_pred, target_names=[\"<=50K\", \">50K\"]))\n",
    "\n",
    "        print(\"\\nConfusion Matrix (Test Set):\")\n",
    "        cm = confusion_matrix(test_labels, test_pred)\n",
    "        print(cm)\n",
    "        print(f\"  True Negatives:  {cm[0,0]}\")\n",
    "        print(f\"  False Positives: {cm[0,1]}\")\n",
    "        print(f\"  False Negatives: {cm[1,0]}\")\n",
    "        print(f\"  True Positives:  {cm[1,1]}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Decision Tree Structure Preview (Top 3 Levels):\")\n",
    "    print(\"=\" * 60)\n",
    "    id3_model.print_tree(max_depth=3)\n",
    "\n",
    "    model_path = Path.cwd() / \"id3_model.pkl\"\n",
    "    id3_model.save_model(model_path)\n",
    "\n",
    "    return {\n",
    "        \"model\": id3_model,\n",
    "        \"train_acc\": train_acc_after,\n",
    "        \"val_acc\": val_acc_after,\n",
    "        \"test_acc\": test_acc if test_features is not None else None,\n",
    "        \"feature_names\": feature_names,\n",
    "        \"num_nodes\": num_nodes_after,\n",
    "        \"tree_depth\": tree_depth_after,\n",
    "    }\n",
    "\n",
    "\n",
    "def save_test_predictions(actual_labels, predicted_labels, algorithm_name):\n",
    "    \"\"\"Save test set predictions to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        actual_labels (np.ndarray): Actual class labels.\n",
    "        predicted_labels (np.ndarray): Predicted class labels.\n",
    "        algorithm_name (str): Name of the algorithm used.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    Path(\"../results\").mkdir(exist_ok=True)\n",
    "    pd.DataFrame({\"actual\": actual_labels, \"predicted\": predicted_labels}).to_csv(\n",
    "        f\"../results/{algorithm_name.lower()}_predictions.csv\", index=False\n",
    "    )\n",
    "    print(f\"✓ {algorithm_name} predictions saved\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the ID3 decision tree training and evaluation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ID3 Decision Tree Implementation\")\n",
    "    print(\"Includes: Discretization, Post-Pruning, Test Set Evaluation\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results = train_and_evaluate_id3(use_pruning=True)\n",
    "\n",
    "    if results is not None:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Training Complete! Final Results Summary\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Training accuracy: {results['train_acc']:.4f}\")\n",
    "        print(f\"Validation accuracy: {results['val_acc']:.4f}\")\n",
    "        if results[\"test_acc\"] is not None:\n",
    "            print(f\"Test accuracy: {results['test_acc']:.4f}\")\n",
    "        print(f\"Decision tree nodes: {results['num_nodes']}\")\n",
    "        print(f\"Decision tree depth: {results['tree_depth']}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"To compare with no pruning, run:\")\n",
    "        print(\"train_and_evaluate_id3(use_pruning=False)\")\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(\"\\nTraining failed. Please check data files and paths\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a5c46ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ID3 Decision Tree Implementation\n",
      "Includes: Discretization, Post-Pruning, Test Set Evaluation\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Starting ID3 Decision Tree Training\n",
      "============================================================\n",
      "============================================================\n",
      "Loading raw data\n",
      "============================================================\n",
      "Training data shape: (32561, 15)\n",
      "Test data shape: (16281, 15)\n",
      "\n",
      "Cleaning data...\n",
      "Training data: 32561 → 30162 (removed 2399 rows)\n",
      "Test data: 16281 → 15060 (removed 1221 rows)\n",
      "\n",
      "Encoding categorical features...\n",
      "✓ Encoding complete for 8 categorical features\n",
      "\n",
      "Discretizing continuous features (n_bins=10)...\n",
      "✓ Discretization complete, each feature divided into 10 bins\n",
      "\n",
      "============================================================\n",
      "Data preparation complete\n",
      "============================================================\n",
      "Number of features: 14\n",
      "  - Continuous features: 6\n",
      "  - Categorical features: 8\n",
      "Discretized: True\n",
      "\n",
      "Data split:\n",
      "  Training set: (24129, 14)\n",
      "  Validation set: (6033, 14)\n",
      "  Test set: (15060, 14)\n",
      "\n",
      "Label distribution (>50K proportion):\n",
      "  Training set: 24.89%\n",
      "  Validation set: 24.90%\n",
      "  Test set: 24.57%\n",
      "============================================================\n",
      "\n",
      "\n",
      "Training ID3 model (max_depth=10, min_samples_split=20)...\n",
      "\n",
      "Tree Statistics:\n",
      "  Number of nodes: 4065\n",
      "  Tree depth: 8\n",
      "\n",
      "Before Pruning:\n",
      "  Training accuracy: 0.8719\n",
      "  Validation accuracy: 0.8145\n",
      "\n",
      "============================================================\n",
      "Test Set Results:\n",
      "============================================================\n",
      "Test accuracy: 0.8079\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.85      0.90      0.88     11360\n",
      "        >50K       0.63      0.52      0.57      3700\n",
      "\n",
      "    accuracy                           0.81     15060\n",
      "   macro avg       0.74      0.71      0.72     15060\n",
      "weighted avg       0.80      0.81      0.80     15060\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[10225  1135]\n",
      " [ 1758  1942]]\n",
      "  True Negatives:  10225\n",
      "  False Positives: 1135\n",
      "  False Negatives: 1758\n",
      "  True Positives:  1942\n",
      "\n",
      "============================================================\n",
      "Decision Tree Structure Preview (Top 3 Levels):\n",
      "============================================================\n",
      "Feature 10: relationship (samples: 24129)\n",
      "  0 => Feature 7: education (samples: 10011)\n",
      "    0 => Feature 9: occupation (samples: 242)\n",
      "      0 => Class: 0 (samples: 5)\n",
      "      10 => Class: 0 (samples: 2)\n",
      "      11 => Class: 0 (samples: 14)\n",
      "      12 => Class: 1 (samples: 1)\n",
      "      13 => Feature 1: fnlwgt (samples: 41)\n",
      "        ... (depth limit reached)\n",
      "      2 => Feature 1: fnlwgt (samples: 83)\n",
      "        ... (depth limit reached)\n",
      "      3 => Class: 0 (samples: 8)\n",
      "      4 => Class: 0 (samples: 14)\n",
      "      5 => Feature 1: fnlwgt (samples: 20)\n",
      "        ... (depth limit reached)\n",
      "      6 => Feature 0: age (samples: 38)\n",
      "        ... (depth limit reached)\n",
      "      7 => Class: 0 (samples: 16)\n",
      "    1 => Feature 0: age (samples: 231)\n",
      "      0 => Class: 0 (samples: 4)\n",
      "      1 => Class: 0 (samples: 19)\n",
      "      2 => Class: 0 (samples: 17)\n",
      "      3 => Class: 0 (samples: 25)\n",
      "      4 => Class: 0 (samples: 28)\n",
      "      5 => Feature 9: occupation (samples: 24)\n",
      "        ... (depth limit reached)\n",
      "      6 => Class: 0 (samples: 18)\n",
      "      7 => Feature 1: fnlwgt (samples: 27)\n",
      "        ... (depth limit reached)\n",
      "      8 => Feature 5: hours_per_week (samples: 28)\n",
      "        ... (depth limit reached)\n",
      "      9 => Feature 11: race (samples: 41)\n",
      "        ... (depth limit reached)\n",
      "    10 => Feature 13: native_country (samples: 198)\n",
      "      1 => Class: 1 (samples: 5)\n",
      "      10 => Class: 1 (samples: 3)\n",
      "      16 => Class: 0 (samples: 2)\n",
      "      18 => Class: 1 (samples: 5)\n",
      "      19 => Class: 1 (samples: 3)\n",
      "      2 => Class: 1 (samples: 5)\n",
      "      22 => Class: 1 (samples: 1)\n",
      "      23 => Class: 1 (samples: 1)\n",
      "      3 => Class: 1 (samples: 1)\n",
      "      34 => Class: 0 (samples: 1)\n",
      "      35 => Class: 1 (samples: 4)\n",
      "      36 => Class: 1 (samples: 1)\n",
      "      38 => Feature 9: occupation (samples: 160)\n",
      "        ... (depth limit reached)\n",
      "      39 => Class: 1 (samples: 1)\n",
      "      4 => Class: 1 (samples: 1)\n",
      "      7 => Class: 1 (samples: 1)\n",
      "      8 => Class: 1 (samples: 1)\n",
      "      9 => Class: 0 (samples: 2)\n",
      "    11 => Feature 0: age (samples: 3232)\n",
      "      0 => Class: 0 (samples: 27)\n",
      "      1 => Feature 9: occupation (samples: 124)\n",
      "        ... (depth limit reached)\n",
      "      2 => Feature 9: occupation (samples: 299)\n",
      "        ... (depth limit reached)\n",
      "      3 => Feature 9: occupation (samples: 270)\n",
      "        ... (depth limit reached)\n",
      "      4 => Feature 5: hours_per_week (samples: 421)\n",
      "        ... (depth limit reached)\n",
      "      5 => Feature 9: occupation (samples: 381)\n",
      "        ... (depth limit reached)\n",
      "      6 => Feature 13: native_country (samples: 352)\n",
      "        ... (depth limit reached)\n",
      "      7 => Feature 5: hours_per_week (samples: 401)\n",
      "        ... (depth limit reached)\n",
      "      8 => Feature 9: occupation (samples: 503)\n",
      "        ... (depth limit reached)\n",
      "      9 => Feature 9: occupation (samples: 454)\n",
      "        ... (depth limit reached)\n",
      "    12 => Feature 9: occupation (samples: 647)\n",
      "      0 => Feature 1: fnlwgt (samples: 23)\n",
      "        ... (depth limit reached)\n",
      "      1 => Class: 1 (samples: 1)\n",
      "      10 => Class: 1 (samples: 6)\n",
      "      11 => Feature 5: hours_per_week (samples: 59)\n",
      "        ... (depth limit reached)\n",
      "      12 => Class: 1 (samples: 14)\n",
      "      13 => Class: 0 (samples: 3)\n",
      "      2 => Class: 1 (samples: 11)\n",
      "      3 => Feature 5: hours_per_week (samples: 238)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 4)\n",
      "      5 => Class: 0 (samples: 2)\n",
      "      6 => Class: 0 (samples: 3)\n",
      "      7 => Class: 0 (samples: 4)\n",
      "      9 => Feature 0: age (samples: 279)\n",
      "        ... (depth limit reached)\n",
      "    13 => Class: 0 (samples: 10)\n",
      "    14 => Feature 0: age (samples: 286)\n",
      "      2 => Class: 0 (samples: 9)\n",
      "      3 => Class: 1 (samples: 15)\n",
      "      4 => Feature 13: native_country (samples: 24)\n",
      "        ... (depth limit reached)\n",
      "      5 => Feature 1: fnlwgt (samples: 47)\n",
      "        ... (depth limit reached)\n",
      "      6 => Feature 11: race (samples: 50)\n",
      "        ... (depth limit reached)\n",
      "      7 => Feature 1: fnlwgt (samples: 44)\n",
      "        ... (depth limit reached)\n",
      "      8 => Feature 5: hours_per_week (samples: 40)\n",
      "        ... (depth limit reached)\n",
      "      9 => Feature 9: occupation (samples: 57)\n",
      "        ... (depth limit reached)\n",
      "    15 => Feature 9: occupation (samples: 1917)\n",
      "      0 => Feature 5: hours_per_week (samples: 118)\n",
      "        ... (depth limit reached)\n",
      "      10 => Feature 0: age (samples: 91)\n",
      "        ... (depth limit reached)\n",
      "      11 => Feature 0: age (samples: 284)\n",
      "        ... (depth limit reached)\n",
      "      12 => Feature 0: age (samples: 70)\n",
      "        ... (depth limit reached)\n",
      "      13 => Feature 6: workclass (samples: 139)\n",
      "        ... (depth limit reached)\n",
      "      2 => Feature 0: age (samples: 424)\n",
      "        ... (depth limit reached)\n",
      "      3 => Feature 6: workclass (samples: 332)\n",
      "        ... (depth limit reached)\n",
      "      4 => Feature 5: hours_per_week (samples: 81)\n",
      "        ... (depth limit reached)\n",
      "      5 => Feature 0: age (samples: 66)\n",
      "        ... (depth limit reached)\n",
      "      6 => Feature 1: fnlwgt (samples: 114)\n",
      "        ... (depth limit reached)\n",
      "      7 => Feature 1: fnlwgt (samples: 71)\n",
      "        ... (depth limit reached)\n",
      "      9 => Feature 0: age (samples: 127)\n",
      "        ... (depth limit reached)\n",
      "    2 => Feature 9: occupation (samples: 70)\n",
      "      0 => Class: 0 (samples: 3)\n",
      "      1 => Class: 0 (samples: 1)\n",
      "      10 => Class: 0 (samples: 1)\n",
      "      11 => Class: 1 (samples: 5)\n",
      "      12 => Class: 0 (samples: 1)\n",
      "      13 => Class: 0 (samples: 15)\n",
      "      2 => Feature 0: age (samples: 22)\n",
      "        ... (depth limit reached)\n",
      "      3 => Class: 0 (samples: 5)\n",
      "      4 => Class: 0 (samples: 2)\n",
      "      5 => Class: 0 (samples: 1)\n",
      "      6 => Class: 0 (samples: 7)\n",
      "      7 => Class: 0 (samples: 5)\n",
      "      9 => Class: 1 (samples: 2)\n",
      "    3 => Feature 9: occupation (samples: 50)\n",
      "      11 => Class: 0 (samples: 3)\n",
      "      13 => Class: 0 (samples: 5)\n",
      "      2 => Class: 0 (samples: 12)\n",
      "      3 => Class: 1 (samples: 2)\n",
      "      4 => Class: 0 (samples: 6)\n",
      "      5 => Class: 0 (samples: 7)\n",
      "      6 => Class: 0 (samples: 7)\n",
      "      7 => Class: 0 (samples: 6)\n",
      "      9 => Class: 0 (samples: 2)\n",
      "    4 => Feature 1: fnlwgt (samples: 104)\n",
      "      0 => Class: 0 (samples: 4)\n",
      "      1 => Class: 0 (samples: 9)\n",
      "      2 => Class: 0 (samples: 4)\n",
      "      3 => Class: 0 (samples: 12)\n",
      "      4 => Class: 0 (samples: 7)\n",
      "      5 => Class: 0 (samples: 8)\n",
      "      6 => Class: 0 (samples: 11)\n",
      "      7 => Class: 0 (samples: 21)\n",
      "      8 => Class: 0 (samples: 16)\n",
      "      9 => Class: 0 (samples: 12)\n",
      "    5 => Feature 9: occupation (samples: 236)\n",
      "      0 => Class: 0 (samples: 4)\n",
      "      10 => Class: 0 (samples: 5)\n",
      "      11 => Class: 0 (samples: 14)\n",
      "      12 => Class: 0 (samples: 2)\n",
      "      13 => Feature 5: hours_per_week (samples: 34)\n",
      "        ... (depth limit reached)\n",
      "      2 => Feature 5: hours_per_week (samples: 65)\n",
      "        ... (depth limit reached)\n",
      "      3 => Class: 0 (samples: 12)\n",
      "      4 => Feature 1: fnlwgt (samples: 38)\n",
      "        ... (depth limit reached)\n",
      "      5 => Class: 0 (samples: 18)\n",
      "      6 => Feature 0: age (samples: 28)\n",
      "        ... (depth limit reached)\n",
      "      7 => Class: 0 (samples: 12)\n",
      "      9 => Class: 0 (samples: 4)\n",
      "    6 => Feature 0: age (samples: 147)\n",
      "      0 => Class: 0 (samples: 1)\n",
      "      1 => Class: 0 (samples: 6)\n",
      "      2 => Class: 0 (samples: 13)\n",
      "      3 => Class: 0 (samples: 12)\n",
      "      4 => Class: 0 (samples: 17)\n",
      "      5 => Class: 0 (samples: 19)\n",
      "      6 => Class: 0 (samples: 9)\n",
      "      7 => Class: 0 (samples: 13)\n",
      "      8 => Feature 1: fnlwgt (samples: 20)\n",
      "        ... (depth limit reached)\n",
      "      9 => Feature 5: hours_per_week (samples: 37)\n",
      "        ... (depth limit reached)\n",
      "    7 => Feature 9: occupation (samples: 306)\n",
      "      0 => Feature 1: fnlwgt (samples: 30)\n",
      "        ... (depth limit reached)\n",
      "      10 => Feature 6: workclass (samples: 22)\n",
      "        ... (depth limit reached)\n",
      "      11 => Feature 5: hours_per_week (samples: 48)\n",
      "        ... (depth limit reached)\n",
      "      12 => Feature 1: fnlwgt (samples: 25)\n",
      "        ... (depth limit reached)\n",
      "      13 => Class: 0 (samples: 11)\n",
      "      2 => Feature 0: age (samples: 50)\n",
      "        ... (depth limit reached)\n",
      "      3 => Feature 0: age (samples: 46)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 7)\n",
      "      5 => Class: 0 (samples: 8)\n",
      "      6 => Class: 0 (samples: 14)\n",
      "      7 => Class: 0 (samples: 9)\n",
      "      9 => Feature 1: fnlwgt (samples: 36)\n",
      "        ... (depth limit reached)\n",
      "    8 => Feature 0: age (samples: 477)\n",
      "      0 => Class: 0 (samples: 2)\n",
      "      1 => Class: 0 (samples: 17)\n",
      "      2 => Feature 1: fnlwgt (samples: 39)\n",
      "        ... (depth limit reached)\n",
      "      3 => Feature 1: fnlwgt (samples: 46)\n",
      "        ... (depth limit reached)\n",
      "      4 => Feature 9: occupation (samples: 61)\n",
      "        ... (depth limit reached)\n",
      "      5 => Feature 9: occupation (samples: 86)\n",
      "        ... (depth limit reached)\n",
      "      6 => Feature 1: fnlwgt (samples: 60)\n",
      "        ... (depth limit reached)\n",
      "      7 => Feature 9: occupation (samples: 68)\n",
      "        ... (depth limit reached)\n",
      "      8 => Feature 9: occupation (samples: 64)\n",
      "        ... (depth limit reached)\n",
      "      9 => Feature 9: occupation (samples: 34)\n",
      "        ... (depth limit reached)\n",
      "    9 => Feature 9: occupation (samples: 1858)\n",
      "      0 => Feature 0: age (samples: 99)\n",
      "        ... (depth limit reached)\n",
      "      10 => Feature 1: fnlwgt (samples: 53)\n",
      "        ... (depth limit reached)\n",
      "      11 => Feature 13: native_country (samples: 328)\n",
      "        ... (depth limit reached)\n",
      "      12 => Feature 1: fnlwgt (samples: 62)\n",
      "        ... (depth limit reached)\n",
      "      13 => Feature 0: age (samples: 28)\n",
      "        ... (depth limit reached)\n",
      "      2 => Feature 5: hours_per_week (samples: 113)\n",
      "        ... (depth limit reached)\n",
      "      3 => Feature 13: native_country (samples: 607)\n",
      "        ... (depth limit reached)\n",
      "      4 => Feature 1: fnlwgt (samples: 38)\n",
      "        ... (depth limit reached)\n",
      "      5 => Class: 0 (samples: 12)\n",
      "      6 => Feature 1: fnlwgt (samples: 30)\n",
      "        ... (depth limit reached)\n",
      "      7 => Feature 5: hours_per_week (samples: 31)\n",
      "        ... (depth limit reached)\n",
      "      9 => Feature 5: hours_per_week (samples: 457)\n",
      "        ... (depth limit reached)\n",
      "  1 => Feature 7: education (samples: 6241)\n",
      "    0 => Feature 9: occupation (samples: 144)\n",
      "      0 => Class: 0 (samples: 6)\n",
      "      10 => Class: 0 (samples: 1)\n",
      "      11 => Class: 0 (samples: 10)\n",
      "      13 => Class: 0 (samples: 14)\n",
      "      2 => Class: 0 (samples: 26)\n",
      "      3 => Class: 0 (samples: 1)\n",
      "      4 => Class: 0 (samples: 3)\n",
      "      5 => Class: 0 (samples: 16)\n",
      "      6 => Class: 0 (samples: 25)\n",
      "      7 => Class: 0 (samples: 37)\n",
      "      8 => Class: 0 (samples: 2)\n",
      "      9 => Class: 0 (samples: 3)\n",
      "    1 => Feature 5: hours_per_week (samples: 155)\n",
      "      0 => Class: 0 (samples: 14)\n",
      "      1 => Class: 0 (samples: 21)\n",
      "      2 => Class: 0 (samples: 5)\n",
      "      3 => Feature 1: fnlwgt (samples: 84)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 11)\n",
      "      5 => Class: 0 (samples: 4)\n",
      "      6 => Class: 0 (samples: 16)\n",
      "    10 => Feature 13: native_country (samples: 70)\n",
      "      1 => Class: 0 (samples: 1)\n",
      "      10 => Class: 0 (samples: 2)\n",
      "      2 => Class: 0 (samples: 2)\n",
      "      21 => Class: 1 (samples: 1)\n",
      "      25 => Class: 1 (samples: 1)\n",
      "      38 => Feature 9: occupation (samples: 59)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 1)\n",
      "      8 => Class: 0 (samples: 3)\n",
      "    11 => Feature 9: occupation (samples: 1874)\n",
      "      0 => Feature 0: age (samples: 321)\n",
      "        ... (depth limit reached)\n",
      "      1 => Class: 0 (samples: 2)\n",
      "      10 => Feature 0: age (samples: 36)\n",
      "        ... (depth limit reached)\n",
      "      11 => Feature 5: hours_per_week (samples: 206)\n",
      "        ... (depth limit reached)\n",
      "      12 => Feature 5: hours_per_week (samples: 32)\n",
      "        ... (depth limit reached)\n",
      "      13 => Feature 1: fnlwgt (samples: 128)\n",
      "        ... (depth limit reached)\n",
      "      2 => Feature 5: hours_per_week (samples: 284)\n",
      "        ... (depth limit reached)\n",
      "      3 => Feature 6: workclass (samples: 145)\n",
      "        ... (depth limit reached)\n",
      "      4 => Feature 0: age (samples: 61)\n",
      "        ... (depth limit reached)\n",
      "      5 => Feature 6: workclass (samples: 112)\n",
      "        ... (depth limit reached)\n",
      "      6 => Feature 0: age (samples: 175)\n",
      "        ... (depth limit reached)\n",
      "      7 => Feature 5: hours_per_week (samples: 301)\n",
      "        ... (depth limit reached)\n",
      "      8 => Class: 0 (samples: 17)\n",
      "      9 => Feature 1: fnlwgt (samples: 54)\n",
      "        ... (depth limit reached)\n",
      "    12 => Feature 5: hours_per_week (samples: 409)\n",
      "      0 => Feature 9: occupation (samples: 28)\n",
      "        ... (depth limit reached)\n",
      "      1 => Feature 0: age (samples: 53)\n",
      "        ... (depth limit reached)\n",
      "      2 => Class: 0 (samples: 12)\n",
      "      3 => Feature 8: marital_status (samples: 151)\n",
      "        ... (depth limit reached)\n",
      "      4 => Feature 0: age (samples: 48)\n",
      "        ... (depth limit reached)\n",
      "      5 => Feature 9: occupation (samples: 55)\n",
      "        ... (depth limit reached)\n",
      "      6 => Feature 1: fnlwgt (samples: 62)\n",
      "        ... (depth limit reached)\n",
      "    13 => Class: 0 (samples: 18)\n",
      "    14 => Feature 0: age (samples: 98)\n",
      "      1 => Class: 0 (samples: 2)\n",
      "      2 => Class: 0 (samples: 18)\n",
      "      3 => Class: 0 (samples: 12)\n",
      "      4 => Class: 1 (samples: 9)\n",
      "      5 => Class: 1 (samples: 13)\n",
      "      6 => Class: 1 (samples: 9)\n",
      "      7 => Class: 1 (samples: 14)\n",
      "      8 => Class: 1 (samples: 13)\n",
      "      9 => Class: 0 (samples: 8)\n",
      "    15 => Feature 0: age (samples: 1364)\n",
      "      0 => Feature 9: occupation (samples: 139)\n",
      "        ... (depth limit reached)\n",
      "      1 => Feature 9: occupation (samples: 208)\n",
      "        ... (depth limit reached)\n",
      "      2 => Feature 1: fnlwgt (samples: 177)\n",
      "        ... (depth limit reached)\n",
      "      3 => Feature 9: occupation (samples: 108)\n",
      "        ... (depth limit reached)\n",
      "      4 => Feature 9: occupation (samples: 126)\n",
      "        ... (depth limit reached)\n",
      "      5 => Feature 9: occupation (samples: 120)\n",
      "        ... (depth limit reached)\n",
      "      6 => Feature 1: fnlwgt (samples: 127)\n",
      "        ... (depth limit reached)\n",
      "      7 => Feature 5: hours_per_week (samples: 116)\n",
      "        ... (depth limit reached)\n",
      "      8 => Feature 5: hours_per_week (samples: 116)\n",
      "        ... (depth limit reached)\n",
      "      9 => Feature 5: hours_per_week (samples: 127)\n",
      "        ... (depth limit reached)\n",
      "    2 => Feature 5: hours_per_week (samples: 61)\n",
      "      0 => Class: 0 (samples: 2)\n",
      "      1 => Class: 0 (samples: 10)\n",
      "      2 => Class: 0 (samples: 1)\n",
      "      3 => Class: 0 (samples: 33)\n",
      "      4 => Class: 0 (samples: 5)\n",
      "      5 => Class: 0 (samples: 5)\n",
      "      6 => Class: 0 (samples: 5)\n",
      "    3 => Class: 0 (samples: 40)\n",
      "    4 => Class: 0 (samples: 47)\n",
      "    5 => Feature 0: age (samples: 87)\n",
      "      0 => Class: 0 (samples: 3)\n",
      "      1 => Class: 0 (samples: 1)\n",
      "      2 => Class: 0 (samples: 1)\n",
      "      3 => Class: 0 (samples: 12)\n",
      "      4 => Class: 0 (samples: 3)\n",
      "      5 => Class: 0 (samples: 2)\n",
      "      6 => Class: 0 (samples: 8)\n",
      "      7 => Class: 0 (samples: 10)\n",
      "      8 => Class: 0 (samples: 16)\n",
      "      9 => Feature 9: occupation (samples: 31)\n",
      "        ... (depth limit reached)\n",
      "    6 => Feature 13: native_country (samples: 75)\n",
      "      1 => Class: 0 (samples: 1)\n",
      "      2 => Class: 0 (samples: 1)\n",
      "      22 => Class: 0 (samples: 2)\n",
      "      25 => Class: 0 (samples: 5)\n",
      "      32 => Class: 0 (samples: 3)\n",
      "      38 => Class: 0 (samples: 58)\n",
      "      5 => Class: 0 (samples: 1)\n",
      "      7 => Class: 0 (samples: 4)\n",
      "    7 => Feature 5: hours_per_week (samples: 253)\n",
      "      0 => Class: 0 (samples: 9)\n",
      "      1 => Class: 0 (samples: 28)\n",
      "      2 => Class: 0 (samples: 5)\n",
      "      3 => Feature 9: occupation (samples: 125)\n",
      "        ... (depth limit reached)\n",
      "      4 => Feature 1: fnlwgt (samples: 27)\n",
      "        ... (depth limit reached)\n",
      "      5 => Feature 1: fnlwgt (samples: 31)\n",
      "        ... (depth limit reached)\n",
      "      6 => Feature 9: occupation (samples: 28)\n",
      "        ... (depth limit reached)\n",
      "    8 => Feature 9: occupation (samples: 285)\n",
      "      0 => Feature 1: fnlwgt (samples: 48)\n",
      "        ... (depth limit reached)\n",
      "      10 => Class: 0 (samples: 12)\n",
      "      11 => Feature 0: age (samples: 24)\n",
      "        ... (depth limit reached)\n",
      "      12 => Feature 0: age (samples: 31)\n",
      "        ... (depth limit reached)\n",
      "      13 => Class: 0 (samples: 6)\n",
      "      2 => Feature 1: fnlwgt (samples: 39)\n",
      "        ... (depth limit reached)\n",
      "      3 => Feature 12: sex (samples: 32)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 7)\n",
      "      5 => Class: 0 (samples: 4)\n",
      "      6 => Class: 0 (samples: 14)\n",
      "      7 => Class: 0 (samples: 28)\n",
      "      8 => Class: 0 (samples: 2)\n",
      "      9 => Feature 6: workclass (samples: 38)\n",
      "        ... (depth limit reached)\n",
      "    9 => Feature 0: age (samples: 1261)\n",
      "      0 => Class: 0 (samples: 2)\n",
      "      1 => Feature 6: workclass (samples: 224)\n",
      "        ... (depth limit reached)\n",
      "      2 => Feature 5: hours_per_week (samples: 244)\n",
      "        ... (depth limit reached)\n",
      "      3 => Feature 5: hours_per_week (samples: 154)\n",
      "        ... (depth limit reached)\n",
      "      4 => Feature 5: hours_per_week (samples: 160)\n",
      "        ... (depth limit reached)\n",
      "      5 => Feature 9: occupation (samples: 128)\n",
      "        ... (depth limit reached)\n",
      "      6 => Feature 5: hours_per_week (samples: 90)\n",
      "        ... (depth limit reached)\n",
      "      7 => Feature 1: fnlwgt (samples: 105)\n",
      "        ... (depth limit reached)\n",
      "      8 => Feature 6: workclass (samples: 79)\n",
      "        ... (depth limit reached)\n",
      "      9 => Feature 1: fnlwgt (samples: 75)\n",
      "        ... (depth limit reached)\n",
      "  2 => Feature 9: occupation (samples: 718)\n",
      "    0 => Feature 8: marital_status (samples: 110)\n",
      "      0 => Class: 0 (samples: 18)\n",
      "      2 => Class: 0 (samples: 17)\n",
      "      3 => Class: 0 (samples: 1)\n",
      "      4 => Class: 0 (samples: 66)\n",
      "      5 => Class: 0 (samples: 6)\n",
      "      6 => Class: 0 (samples: 2)\n",
      "    1 => Class: 0 (samples: 1)\n",
      "    10 => Class: 0 (samples: 12)\n",
      "    11 => Feature 6: workclass (samples: 78)\n",
      "      2 => Class: 0 (samples: 71)\n",
      "      3 => Class: 0 (samples: 2)\n",
      "      4 => Class: 0 (samples: 5)\n",
      "    12 => Class: 0 (samples: 16)\n",
      "    13 => Feature 13: native_country (samples: 24)\n",
      "      12 => Class: 0 (samples: 1)\n",
      "      18 => Class: 1 (samples: 1)\n",
      "      25 => Class: 0 (samples: 3)\n",
      "      38 => Class: 0 (samples: 17)\n",
      "      5 => Class: 0 (samples: 1)\n",
      "      6 => Class: 1 (samples: 1)\n",
      "    2 => Feature 0: age (samples: 76)\n",
      "      0 => Class: 0 (samples: 7)\n",
      "      1 => Class: 0 (samples: 15)\n",
      "      2 => Class: 0 (samples: 15)\n",
      "      3 => Class: 0 (samples: 9)\n",
      "      4 => Class: 0 (samples: 7)\n",
      "      5 => Class: 0 (samples: 6)\n",
      "      6 => Class: 0 (samples: 3)\n",
      "      7 => Class: 0 (samples: 6)\n",
      "      8 => Class: 0 (samples: 6)\n",
      "      9 => Class: 0 (samples: 2)\n",
      "    3 => Feature 7: education (samples: 39)\n",
      "      1 => Class: 1 (samples: 1)\n",
      "      11 => Class: 0 (samples: 10)\n",
      "      12 => Class: 1 (samples: 1)\n",
      "      15 => Class: 0 (samples: 12)\n",
      "      6 => Class: 0 (samples: 1)\n",
      "      8 => Class: 1 (samples: 1)\n",
      "      9 => Class: 0 (samples: 13)\n",
      "    4 => Feature 2: education_num (samples: 28)\n",
      "      0 => Class: 0 (samples: 15)\n",
      "      1 => Class: 0 (samples: 2)\n",
      "      2 => Class: 0 (samples: 7)\n",
      "      3 => Class: 0 (samples: 3)\n",
      "      4 => Class: 1 (samples: 1)\n",
      "    5 => Feature 1: fnlwgt (samples: 69)\n",
      "      0 => Class: 0 (samples: 2)\n",
      "      1 => Class: 0 (samples: 3)\n",
      "      2 => Class: 0 (samples: 6)\n",
      "      3 => Class: 0 (samples: 6)\n",
      "      4 => Class: 0 (samples: 5)\n",
      "      5 => Class: 0 (samples: 2)\n",
      "      6 => Class: 0 (samples: 12)\n",
      "      7 => Class: 0 (samples: 8)\n",
      "      8 => Class: 0 (samples: 12)\n",
      "      9 => Class: 0 (samples: 13)\n",
      "    6 => Class: 0 (samples: 64)\n",
      "    7 => Class: 0 (samples: 145)\n",
      "    8 => Class: 0 (samples: 15)\n",
      "    9 => Feature 7: education (samples: 41)\n",
      "      1 => Class: 0 (samples: 1)\n",
      "      10 => Class: 1 (samples: 2)\n",
      "      11 => Class: 0 (samples: 5)\n",
      "      12 => Class: 0 (samples: 6)\n",
      "      14 => Class: 0 (samples: 3)\n",
      "      15 => Class: 0 (samples: 8)\n",
      "      5 => Class: 0 (samples: 1)\n",
      "      7 => Class: 0 (samples: 3)\n",
      "      9 => Class: 0 (samples: 12)\n",
      "  3 => Feature 0: age (samples: 3504)\n",
      "    0 => Feature 8: marital_status (samples: 1442)\n",
      "      0 => Class: 0 (samples: 3)\n",
      "      2 => Class: 0 (samples: 7)\n",
      "      3 => Class: 0 (samples: 7)\n",
      "      4 => Class: 0 (samples: 1417)\n",
      "      5 => Class: 0 (samples: 8)\n",
      "    1 => Feature 9: occupation (samples: 901)\n",
      "      0 => Feature 6: workclass (samples: 172)\n",
      "        ... (depth limit reached)\n",
      "      10 => Class: 0 (samples: 17)\n",
      "      11 => Feature 6: workclass (samples: 126)\n",
      "        ... (depth limit reached)\n",
      "      12 => Class: 0 (samples: 28)\n",
      "      13 => Class: 0 (samples: 33)\n",
      "      2 => Feature 13: native_country (samples: 88)\n",
      "        ... (depth limit reached)\n",
      "      3 => Class: 0 (samples: 66)\n",
      "      4 => Feature 1: fnlwgt (samples: 23)\n",
      "        ... (depth limit reached)\n",
      "      5 => Class: 0 (samples: 83)\n",
      "      6 => Class: 0 (samples: 60)\n",
      "      7 => Feature 1: fnlwgt (samples: 119)\n",
      "        ... (depth limit reached)\n",
      "      8 => Class: 0 (samples: 6)\n",
      "      9 => Feature 7: education (samples: 80)\n",
      "        ... (depth limit reached)\n",
      "    2 => Feature 9: occupation (samples: 426)\n",
      "      0 => Feature 1: fnlwgt (samples: 78)\n",
      "        ... (depth limit reached)\n",
      "      10 => Class: 0 (samples: 11)\n",
      "      11 => Class: 0 (samples: 49)\n",
      "      12 => Class: 0 (samples: 23)\n",
      "      13 => Class: 0 (samples: 21)\n",
      "      2 => Class: 0 (samples: 38)\n",
      "      3 => Feature 5: hours_per_week (samples: 35)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 15)\n",
      "      5 => Class: 0 (samples: 40)\n",
      "      6 => Class: 0 (samples: 23)\n",
      "      7 => Class: 0 (samples: 51)\n",
      "      9 => Feature 13: native_country (samples: 42)\n",
      "        ... (depth limit reached)\n",
      "    3 => Feature 8: marital_status (samples: 194)\n",
      "      0 => Feature 1: fnlwgt (samples: 30)\n",
      "        ... (depth limit reached)\n",
      "      1 => Class: 0 (samples: 1)\n",
      "      2 => Class: 0 (samples: 10)\n",
      "      3 => Class: 0 (samples: 2)\n",
      "      4 => Class: 0 (samples: 150)\n",
      "      5 => Class: 0 (samples: 1)\n",
      "    4 => Feature 7: education (samples: 190)\n",
      "      0 => Class: 0 (samples: 7)\n",
      "      1 => Class: 0 (samples: 7)\n",
      "      11 => Feature 9: occupation (samples: 87)\n",
      "        ... (depth limit reached)\n",
      "      12 => Class: 0 (samples: 7)\n",
      "      13 => Class: 0 (samples: 1)\n",
      "      14 => Class: 0 (samples: 2)\n",
      "      15 => Feature 1: fnlwgt (samples: 28)\n",
      "        ... (depth limit reached)\n",
      "      2 => Class: 0 (samples: 1)\n",
      "      3 => Class: 0 (samples: 1)\n",
      "      4 => Class: 0 (samples: 1)\n",
      "      5 => Class: 0 (samples: 4)\n",
      "      6 => Class: 0 (samples: 2)\n",
      "      7 => Class: 0 (samples: 6)\n",
      "      8 => Class: 0 (samples: 13)\n",
      "      9 => Feature 1: fnlwgt (samples: 23)\n",
      "        ... (depth limit reached)\n",
      "    5 => Feature 1: fnlwgt (samples: 131)\n",
      "      0 => Class: 0 (samples: 8)\n",
      "      1 => Class: 0 (samples: 14)\n",
      "      2 => Class: 0 (samples: 18)\n",
      "      3 => Class: 0 (samples: 11)\n",
      "      4 => Class: 0 (samples: 16)\n",
      "      5 => Class: 0 (samples: 16)\n",
      "      6 => Class: 0 (samples: 12)\n",
      "      7 => Class: 0 (samples: 11)\n",
      "      8 => Class: 0 (samples: 11)\n",
      "      9 => Class: 0 (samples: 14)\n",
      "    6 => Feature 8: marital_status (samples: 83)\n",
      "      0 => Feature 1: fnlwgt (samples: 36)\n",
      "        ... (depth limit reached)\n",
      "      2 => Class: 0 (samples: 4)\n",
      "      3 => Class: 0 (samples: 1)\n",
      "      4 => Class: 0 (samples: 38)\n",
      "      5 => Class: 0 (samples: 4)\n",
      "    7 => Feature 6: workclass (samples: 73)\n",
      "      0 => Class: 0 (samples: 2)\n",
      "      1 => Class: 0 (samples: 7)\n",
      "      2 => Feature 5: hours_per_week (samples: 52)\n",
      "        ... (depth limit reached)\n",
      "      3 => Class: 1 (samples: 1)\n",
      "      4 => Class: 0 (samples: 7)\n",
      "      5 => Class: 0 (samples: 4)\n",
      "    8 => Feature 7: education (samples: 40)\n",
      "      0 => Class: 0 (samples: 2)\n",
      "      1 => Class: 0 (samples: 3)\n",
      "      11 => Class: 0 (samples: 17)\n",
      "      12 => Class: 0 (samples: 3)\n",
      "      15 => Class: 0 (samples: 4)\n",
      "      2 => Class: 0 (samples: 2)\n",
      "      5 => Class: 0 (samples: 3)\n",
      "      7 => Class: 0 (samples: 1)\n",
      "      8 => Class: 0 (samples: 1)\n",
      "      9 => Class: 0 (samples: 4)\n",
      "    9 => Feature 5: hours_per_week (samples: 24)\n",
      "      0 => Class: 0 (samples: 6)\n",
      "      1 => Class: 0 (samples: 1)\n",
      "      2 => Class: 0 (samples: 1)\n",
      "      3 => Class: 0 (samples: 12)\n",
      "      4 => Class: 1 (samples: 1)\n",
      "      6 => Class: 0 (samples: 3)\n",
      "  4 => Feature 7: education (samples: 2551)\n",
      "    0 => Feature 9: occupation (samples: 84)\n",
      "      0 => Class: 0 (samples: 11)\n",
      "      11 => Class: 0 (samples: 10)\n",
      "      13 => Class: 0 (samples: 7)\n",
      "      2 => Class: 0 (samples: 9)\n",
      "      3 => Class: 0 (samples: 3)\n",
      "      4 => Class: 0 (samples: 8)\n",
      "      5 => Class: 0 (samples: 3)\n",
      "      6 => Class: 0 (samples: 10)\n",
      "      7 => Class: 0 (samples: 21)\n",
      "      9 => Class: 0 (samples: 2)\n",
      "    1 => Feature 13: native_country (samples: 103)\n",
      "      10 => Class: 0 (samples: 1)\n",
      "      11 => Class: 1 (samples: 1)\n",
      "      12 => Class: 0 (samples: 1)\n",
      "      18 => Class: 0 (samples: 1)\n",
      "      22 => Class: 0 (samples: 1)\n",
      "      25 => Class: 0 (samples: 2)\n",
      "      26 => Class: 0 (samples: 2)\n",
      "      30 => Class: 0 (samples: 1)\n",
      "      32 => Class: 0 (samples: 3)\n",
      "      38 => Feature 9: occupation (samples: 90)\n",
      "        ... (depth limit reached)\n",
      "    10 => Feature 12: sex (samples: 20)\n",
      "      0 => Class: 0 (samples: 13)\n",
      "      1 => Class: 1 (samples: 7)\n",
      "    11 => Feature 0: age (samples: 970)\n",
      "      0 => Class: 0 (samples: 22)\n",
      "      1 => Feature 13: native_country (samples: 85)\n",
      "        ... (depth limit reached)\n",
      "      2 => Feature 6: workclass (samples: 82)\n",
      "        ... (depth limit reached)\n",
      "      3 => Feature 1: fnlwgt (samples: 87)\n",
      "        ... (depth limit reached)\n",
      "      4 => Feature 1: fnlwgt (samples: 135)\n",
      "        ... (depth limit reached)\n",
      "      5 => Feature 1: fnlwgt (samples: 134)\n",
      "        ... (depth limit reached)\n",
      "      6 => Feature 9: occupation (samples: 110)\n",
      "        ... (depth limit reached)\n",
      "      7 => Feature 9: occupation (samples: 118)\n",
      "        ... (depth limit reached)\n",
      "      8 => Feature 5: hours_per_week (samples: 109)\n",
      "        ... (depth limit reached)\n",
      "      9 => Feature 5: hours_per_week (samples: 88)\n",
      "        ... (depth limit reached)\n",
      "    12 => Feature 9: occupation (samples: 105)\n",
      "      0 => Class: 0 (samples: 3)\n",
      "      10 => Class: 0 (samples: 1)\n",
      "      11 => Class: 1 (samples: 4)\n",
      "      2 => Class: 0 (samples: 1)\n",
      "      3 => Feature 5: hours_per_week (samples: 30)\n",
      "        ... (depth limit reached)\n",
      "      5 => Class: 0 (samples: 1)\n",
      "      6 => Class: 0 (samples: 1)\n",
      "      7 => Class: 0 (samples: 3)\n",
      "      9 => Feature 1: fnlwgt (samples: 61)\n",
      "        ... (depth limit reached)\n",
      "    13 => Class: 0 (samples: 1)\n",
      "    14 => Class: 0 (samples: 17)\n",
      "    15 => Feature 9: occupation (samples: 584)\n",
      "      0 => Feature 1: fnlwgt (samples: 209)\n",
      "        ... (depth limit reached)\n",
      "      10 => Class: 0 (samples: 10)\n",
      "      11 => Feature 5: hours_per_week (samples: 64)\n",
      "        ... (depth limit reached)\n",
      "      12 => Class: 0 (samples: 25)\n",
      "      13 => Class: 0 (samples: 13)\n",
      "      2 => Feature 0: age (samples: 37)\n",
      "        ... (depth limit reached)\n",
      "      3 => Feature 6: workclass (samples: 80)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 6)\n",
      "      5 => Class: 0 (samples: 8)\n",
      "      6 => Feature 0: age (samples: 24)\n",
      "        ... (depth limit reached)\n",
      "      7 => Class: 0 (samples: 67)\n",
      "      8 => Class: 0 (samples: 3)\n",
      "      9 => Feature 6: workclass (samples: 38)\n",
      "        ... (depth limit reached)\n",
      "    2 => Class: 0 (samples: 26)\n",
      "    3 => Class: 0 (samples: 14)\n",
      "    4 => Class: 0 (samples: 29)\n",
      "    5 => Class: 0 (samples: 53)\n",
      "    6 => Feature 9: occupation (samples: 50)\n",
      "      0 => Class: 0 (samples: 1)\n",
      "      11 => Class: 0 (samples: 5)\n",
      "      13 => Class: 1 (samples: 1)\n",
      "      2 => Class: 0 (samples: 7)\n",
      "      3 => Class: 0 (samples: 1)\n",
      "      4 => Class: 0 (samples: 1)\n",
      "      5 => Class: 0 (samples: 4)\n",
      "      6 => Class: 0 (samples: 8)\n",
      "      7 => Class: 0 (samples: 18)\n",
      "      8 => Class: 0 (samples: 4)\n",
      "    7 => Feature 9: occupation (samples: 99)\n",
      "      0 => Feature 5: hours_per_week (samples: 26)\n",
      "        ... (depth limit reached)\n",
      "      11 => Class: 0 (samples: 12)\n",
      "      12 => Class: 0 (samples: 3)\n",
      "      13 => Class: 0 (samples: 3)\n",
      "      2 => Class: 0 (samples: 4)\n",
      "      3 => Class: 0 (samples: 14)\n",
      "      4 => Class: 0 (samples: 1)\n",
      "      5 => Class: 0 (samples: 1)\n",
      "      6 => Class: 0 (samples: 5)\n",
      "      7 => Class: 0 (samples: 11)\n",
      "      9 => Class: 0 (samples: 19)\n",
      "    8 => Feature 9: occupation (samples: 145)\n",
      "      0 => Feature 5: hours_per_week (samples: 27)\n",
      "        ... (depth limit reached)\n",
      "      10 => Class: 0 (samples: 2)\n",
      "      11 => Class: 0 (samples: 13)\n",
      "      12 => Class: 0 (samples: 14)\n",
      "      13 => Class: 0 (samples: 1)\n",
      "      2 => Class: 0 (samples: 12)\n",
      "      3 => Class: 0 (samples: 13)\n",
      "      5 => Class: 0 (samples: 3)\n",
      "      6 => Class: 0 (samples: 7)\n",
      "      7 => Class: 0 (samples: 16)\n",
      "      9 => Feature 0: age (samples: 37)\n",
      "        ... (depth limit reached)\n",
      "    9 => Feature 9: occupation (samples: 251)\n",
      "      0 => Feature 0: age (samples: 38)\n",
      "        ... (depth limit reached)\n",
      "      10 => Class: 0 (samples: 3)\n",
      "      11 => Feature 0: age (samples: 25)\n",
      "        ... (depth limit reached)\n",
      "      12 => Class: 0 (samples: 13)\n",
      "      13 => Class: 0 (samples: 2)\n",
      "      2 => Class: 0 (samples: 7)\n",
      "      3 => Feature 1: fnlwgt (samples: 57)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 2)\n",
      "      6 => Class: 0 (samples: 3)\n",
      "      7 => Class: 0 (samples: 8)\n",
      "      8 => Class: 0 (samples: 2)\n",
      "      9 => Feature 1: fnlwgt (samples: 91)\n",
      "        ... (depth limit reached)\n",
      "  5 => Feature 7: education (samples: 1104)\n",
      "    0 => Class: 0 (samples: 17)\n",
      "    1 => Feature 0: age (samples: 20)\n",
      "      1 => Class: 0 (samples: 1)\n",
      "      2 => Class: 0 (samples: 3)\n",
      "      3 => Class: 0 (samples: 3)\n",
      "      4 => Class: 0 (samples: 1)\n",
      "      6 => Class: 0 (samples: 2)\n",
      "      7 => Class: 0 (samples: 6)\n",
      "      8 => Class: 0 (samples: 3)\n",
      "      9 => Class: 0 (samples: 1)\n",
      "    10 => Class: 1 (samples: 16)\n",
      "    11 => Feature 9: occupation (samples: 359)\n",
      "      0 => Feature 6: workclass (samples: 118)\n",
      "        ... (depth limit reached)\n",
      "      10 => Class: 0 (samples: 2)\n",
      "      11 => Feature 0: age (samples: 45)\n",
      "        ... (depth limit reached)\n",
      "      12 => Class: 0 (samples: 9)\n",
      "      13 => Class: 0 (samples: 7)\n",
      "      2 => Class: 1 (samples: 9)\n",
      "      3 => Feature 1: fnlwgt (samples: 43)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 6)\n",
      "      5 => Class: 0 (samples: 7)\n",
      "      6 => Feature 1: fnlwgt (samples: 34)\n",
      "        ... (depth limit reached)\n",
      "      7 => Feature 1: fnlwgt (samples: 60)\n",
      "        ... (depth limit reached)\n",
      "      8 => Class: 0 (samples: 4)\n",
      "      9 => Class: 0 (samples: 15)\n",
      "    12 => Feature 1: fnlwgt (samples: 81)\n",
      "      0 => Class: 1 (samples: 10)\n",
      "      1 => Class: 1 (samples: 11)\n",
      "      2 => Class: 1 (samples: 5)\n",
      "      3 => Class: 1 (samples: 7)\n",
      "      4 => Class: 1 (samples: 11)\n",
      "      5 => Class: 1 (samples: 12)\n",
      "      6 => Class: 1 (samples: 12)\n",
      "      7 => Class: 0 (samples: 2)\n",
      "      8 => Class: 1 (samples: 3)\n",
      "      9 => Class: 1 (samples: 8)\n",
      "    13 => Class: 0 (samples: 2)\n",
      "    14 => Class: 1 (samples: 19)\n",
      "    15 => Feature 9: occupation (samples: 218)\n",
      "      0 => Feature 1: fnlwgt (samples: 78)\n",
      "        ... (depth limit reached)\n",
      "      10 => Class: 1 (samples: 1)\n",
      "      11 => Feature 0: age (samples: 26)\n",
      "        ... (depth limit reached)\n",
      "      12 => Class: 0 (samples: 9)\n",
      "      13 => Class: 0 (samples: 3)\n",
      "      2 => Class: 0 (samples: 3)\n",
      "      3 => Feature 0: age (samples: 40)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 2)\n",
      "      5 => Class: 0 (samples: 2)\n",
      "      6 => Class: 0 (samples: 4)\n",
      "      7 => Feature 0: age (samples: 26)\n",
      "        ... (depth limit reached)\n",
      "      9 => Feature 1: fnlwgt (samples: 24)\n",
      "        ... (depth limit reached)\n",
      "    2 => Class: 0 (samples: 12)\n",
      "    3 => Class: 0 (samples: 1)\n",
      "    4 => Class: 0 (samples: 5)\n",
      "    5 => Class: 0 (samples: 17)\n",
      "    6 => Class: 0 (samples: 15)\n",
      "    7 => Feature 0: age (samples: 54)\n",
      "      0 => Class: 1 (samples: 1)\n",
      "      1 => Class: 0 (samples: 6)\n",
      "      2 => Class: 0 (samples: 8)\n",
      "      3 => Class: 1 (samples: 6)\n",
      "      4 => Class: 1 (samples: 11)\n",
      "      5 => Class: 1 (samples: 10)\n",
      "      6 => Class: 0 (samples: 5)\n",
      "      7 => Class: 1 (samples: 3)\n",
      "      8 => Class: 0 (samples: 3)\n",
      "      9 => Class: 1 (samples: 1)\n",
      "    8 => Feature 9: occupation (samples: 55)\n",
      "      0 => Class: 0 (samples: 13)\n",
      "      11 => Class: 1 (samples: 1)\n",
      "      12 => Class: 1 (samples: 8)\n",
      "      13 => Class: 1 (samples: 1)\n",
      "      2 => Class: 0 (samples: 1)\n",
      "      3 => Class: 1 (samples: 7)\n",
      "      5 => Class: 0 (samples: 1)\n",
      "      6 => Class: 0 (samples: 1)\n",
      "      7 => Class: 0 (samples: 6)\n",
      "      9 => Class: 1 (samples: 16)\n",
      "    9 => Feature 13: native_country (samples: 213)\n",
      "      10 => Class: 1 (samples: 3)\n",
      "      13 => Class: 1 (samples: 1)\n",
      "      19 => Class: 1 (samples: 1)\n",
      "      23 => Class: 1 (samples: 1)\n",
      "      25 => Class: 0 (samples: 1)\n",
      "      29 => Class: 1 (samples: 6)\n",
      "      30 => Class: 0 (samples: 1)\n",
      "      32 => Class: 0 (samples: 1)\n",
      "      34 => Class: 0 (samples: 3)\n",
      "      35 => Class: 0 (samples: 2)\n",
      "      36 => Class: 0 (samples: 1)\n",
      "      38 => Feature 0: age (samples: 189)\n",
      "        ... (depth limit reached)\n",
      "      4 => Class: 0 (samples: 2)\n",
      "      8 => Class: 1 (samples: 1)\n",
      "\n",
      "============================================================\n",
      "Training Complete! Final Results Summary\n",
      "============================================================\n",
      "Training accuracy: 0.8719\n",
      "Validation accuracy: 0.8145\n",
      "Test accuracy: 0.8079\n",
      "Decision tree nodes: 4065\n",
      "Decision tree depth: 8\n",
      "\n",
      "============================================================\n",
      "To compare with no pruning, run:\n",
      "train_and_evaluate_id3(use_pruning=False)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from hw1_part2_preprocessing import UnifiedDataPreprocessor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class ID3DecisionTree:\n",
    "    \"\"\"Decision tree classifier using the ID3 algorithm with optional pruning.\n",
    "\n",
    "    Attributes:\n",
    "        max_depth (int, optional): Maximum depth of the tree. Defaults to None.\n",
    "        min_samples_split (int): Minimum number of samples required to split. Defaults to 2.\n",
    "        tree (dict): The constructed decision tree.\n",
    "        feature_names (list): Names of features used in the tree.\n",
    "        majority_class (int): Majority class for handling unknown values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        \"\"\"Initialize the ID3 decision tree.\n",
    "\n",
    "        Args:\n",
    "            max_depth (int, optional): Maximum depth of the tree. Defaults to None.\n",
    "            min_samples_split (int): Minimum number of samples required to split. Defaults to 2.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "        self.feature_names = None\n",
    "        self.majority_class = None\n",
    "\n",
    "    def compute_entropy(self, labels):\n",
    "        \"\"\"Calculate the entropy of a label set.\n",
    "\n",
    "        Args:\n",
    "            labels (np.ndarray): Array of class labels.\n",
    "\n",
    "        Returns:\n",
    "            float: Entropy value of the label set.\n",
    "        \"\"\"\n",
    "        if len(labels) == 0:\n",
    "            return 0\n",
    "        _, counts = np.unique(labels, return_counts=True)\n",
    "        probabilities = counts / len(labels)\n",
    "        return -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "\n",
    "    def compute_information_gain(self, features, labels, feature_idx):\n",
    "        \"\"\"Calculate the information gain for a specific feature.\n",
    "\n",
    "        Args:\n",
    "            features (np.ndarray): Feature matrix.\n",
    "            labels (np.ndarray): Array of class labels.\n",
    "            feature_idx (int): Index of the feature to evaluate.\n",
    "\n",
    "        Returns:\n",
    "            float: Information gain for the specified feature.\n",
    "        \"\"\"\n",
    "        parent_entropy = self.compute_entropy(labels)\n",
    "        values, counts = np.unique(features[:, feature_idx], return_counts=True)\n",
    "        weighted_entropy = 0\n",
    "\n",
    "        total_samples = len(labels)\n",
    "        for value, count in zip(values, counts):\n",
    "            mask = features[:, feature_idx] == value\n",
    "            child_samples = len(labels[mask])\n",
    "            if child_samples > 0:\n",
    "                child_entropy = self.compute_entropy(labels[mask])\n",
    "                weighted_entropy += (child_samples / total_samples) * child_entropy\n",
    "\n",
    "        return parent_entropy - weighted_entropy\n",
    "\n",
    "    def build_tree(self, features, labels, depth=0, features_to_consider=None):\n",
    "        \"\"\"Recursively build the decision tree.\n",
    "\n",
    "        Args:\n",
    "            features (np.ndarray): Feature matrix.\n",
    "            labels (np.ndarray): Array of class labels.\n",
    "            depth (int): Current depth of the tree. Defaults to 0.\n",
    "            features_to_consider (list, optional): List of feature indices to consider. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            dict: Node of the decision tree.\n",
    "        \"\"\"\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(unique_labels) == 1:\n",
    "            return {\"class\": int(unique_labels[0]), \"samples\": len(labels)}\n",
    "\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            majority_class = int(np.argmax(np.bincount(labels)))\n",
    "            return {\"class\": majority_class, \"samples\": len(labels)}\n",
    "\n",
    "        if len(labels) < self.min_samples_split:\n",
    "            majority_class = int(np.argmax(np.bincount(labels)))\n",
    "            return {\"class\": majority_class, \"samples\": len(labels)}\n",
    "\n",
    "        if features_to_consider is None:\n",
    "            features_to_consider = list(range(features.shape[1]))\n",
    "\n",
    "        if len(features_to_consider) == 0:\n",
    "            majority_class = int(np.argmax(np.bincount(labels)))\n",
    "            return {\"class\": majority_class, \"samples\": len(labels)}\n",
    "\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        for feature in features_to_consider:\n",
    "            gain = self.compute_information_gain(features, labels, feature)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "\n",
    "        if best_feature is None or best_gain < 1e-5:\n",
    "            majority_class = int(np.argmax(np.bincount(labels)))\n",
    "            return {\"class\": majority_class, \"samples\": len(labels)}\n",
    "\n",
    "        children = {}\n",
    "        feature_values = np.unique(features[:, best_feature])\n",
    "        for value in feature_values:\n",
    "            mask = features[:, best_feature] == value\n",
    "            child_features, child_labels = features[mask], labels[mask]\n",
    "            if len(child_labels) == 0:\n",
    "                majority_class = int(np.argmax(np.bincount(labels)))\n",
    "                children[value] = {\"class\": majority_class, \"samples\": 0}\n",
    "            else:\n",
    "                new_features = [f for f in features_to_consider if f != best_feature]\n",
    "                children[value] = self.build_tree(child_features, child_labels, depth + 1, new_features)\n",
    "\n",
    "        return {\n",
    "            \"feature\": best_feature,\n",
    "            \"children\": children,\n",
    "            \"samples\": len(labels),\n",
    "            \"class\": int(np.argmax(np.bincount(labels))),\n",
    "        }\n",
    "\n",
    "    def fit(self, features, labels):\n",
    "        \"\"\"Train the decision tree model.\n",
    "\n",
    "        Args:\n",
    "            features (np.ndarray): Training feature matrix.\n",
    "            labels (np.ndarray): Training class labels.\n",
    "\n",
    "        Returns:\n",
    "            ID3DecisionTree: Trained model instance.\n",
    "        \"\"\"\n",
    "        self.majority_class = int(np.argmax(np.bincount(labels)))\n",
    "        self.tree = self.build_tree(features, labels)\n",
    "        if self.feature_names is None:\n",
    "            self.feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n",
    "        return self\n",
    "\n",
    "    def predict_one(self, sample, node):\n",
    "        \"\"\"Predict the class for a single sample.\n",
    "\n",
    "        Args:\n",
    "            sample (np.ndarray): Single feature vector.\n",
    "            node (dict): Current node in the decision tree.\n",
    "\n",
    "        Returns:\n",
    "            int: Predicted class label.\n",
    "        \"\"\"\n",
    "        if \"class\" in node and \"children\" not in node:\n",
    "            return node[\"class\"]\n",
    "\n",
    "        if \"children\" in node:\n",
    "            feature_idx = node[\"feature\"]\n",
    "            feature_value = sample[feature_idx]\n",
    "            if feature_value in node[\"children\"]:\n",
    "                return self.predict_one(sample, node[\"children\"][feature_value])\n",
    "            return node.get(\"class\", self.majority_class)\n",
    "\n",
    "        return node.get(\"class\", self.majority_class)\n",
    "\n",
    "    def predict(self, features):\n",
    "        \"\"\"Predict classes for a dataset.\n",
    "\n",
    "        Args:\n",
    "            features (np.ndarray): Feature matrix to predict.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Predicted class labels.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the model is not fitted.\n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            raise ValueError(\"Model must be fitted before prediction\")\n",
    "\n",
    "        predictions = []\n",
    "        for i in range(features.shape[0]):\n",
    "            try:\n",
    "                pred = self.predict_one(features[i], self.tree)\n",
    "                predictions.append(pred)\n",
    "            except Exception as e:\n",
    "                print(f\"Prediction error for sample {i}: {e}\")\n",
    "                predictions.append(self.majority_class)\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def prune(self, validation_features, validation_labels):\n",
    "        \"\"\"Apply post-pruning to the decision tree using validation data.\n",
    "\n",
    "        Args:\n",
    "            validation_features (np.ndarray): Validation feature matrix.\n",
    "            validation_labels (np.ndarray): Validation class labels.\n",
    "\n",
    "        Returns:\n",
    "            ID3DecisionTree: Pruned model instance.\n",
    "        \"\"\"\n",
    "        print(\"\\nStarting post-pruning...\")\n",
    "        initial_acc = accuracy_score(validation_labels, self.predict(validation_features))\n",
    "        print(f\"Validation accuracy before pruning: {initial_acc:.4f}\")\n",
    "\n",
    "        self.tree = self._prune_node(self.tree, validation_features, validation_labels)\n",
    "\n",
    "        final_acc = accuracy_score(validation_labels, self.predict(validation_features))\n",
    "        print(f\"Validation accuracy after pruning: {final_acc:.4f}\")\n",
    "        print(f\"Accuracy change: {final_acc - initial_acc:+.4f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _prune_node(self, node, validation_features, validation_labels):\n",
    "        \"\"\"Recursively prune the decision tree.\n",
    "\n",
    "        Args:\n",
    "            node (dict): Current node in the decision tree.\n",
    "            validation_features (np.ndarray): Validation feature matrix.\n",
    "            validation_labels (np.ndarray): Validation class labels.\n",
    "\n",
    "        Returns:\n",
    "            dict: Pruned node.\n",
    "        \"\"\"\n",
    "        if \"children\" not in node:\n",
    "            return node\n",
    "\n",
    "        for value in list(node[\"children\"].keys()):\n",
    "            node[\"children\"][value] = self._prune_node(\n",
    "                node[\"children\"][value], validation_features, validation_labels\n",
    "            )\n",
    "\n",
    "        pred_before = self.predict(validation_features)\n",
    "        error_before = np.mean(pred_before != validation_labels)\n",
    "\n",
    "        original_children = node[\"children\"]\n",
    "        node[\"children\"] = {}\n",
    "        pred_after = self.predict(validation_features)\n",
    "        error_after = np.mean(pred_after != validation_labels)\n",
    "\n",
    "        if error_after <= error_before:\n",
    "            del node[\"children\"]\n",
    "            del node[\"feature\"]\n",
    "            return node\n",
    "\n",
    "        node[\"children\"] = original_children\n",
    "        return node\n",
    "\n",
    "    def count_nodes(self):\n",
    "        \"\"\"Count the total number of nodes in the tree.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of nodes in the tree.\n",
    "        \"\"\"\n",
    "        def count_recursive(node):\n",
    "            if \"children\" not in node:\n",
    "                return 1\n",
    "            return 1 + sum(count_recursive(child) for child in node[\"children\"].values())\n",
    "\n",
    "        return count_recursive(self.tree) if self.tree else 0\n",
    "\n",
    "    def get_depth(self):\n",
    "        \"\"\"Calculate the depth of the tree.\n",
    "\n",
    "        Returns:\n",
    "            int: Depth of the decision tree.\n",
    "        \"\"\"\n",
    "        def depth_recursive(node):\n",
    "            if \"children\" not in node:\n",
    "                return 1\n",
    "            if not node[\"children\"]:\n",
    "                return 1\n",
    "            return 1 + max(depth_recursive(child) for child in node[\"children\"].values())\n",
    "\n",
    "        return depth_recursive(self.tree) if self.tree else 0\n",
    "\n",
    "    def print_tree(self, max_depth=None):\n",
    "        \"\"\"Print the structure of the decision tree.\n",
    "\n",
    "        Args:\n",
    "            max_depth (int, optional): Maximum depth to print. Defaults to None.\n",
    "        \"\"\"\n",
    "        def print_recursive(node, depth=0, prefix=\"\"):\n",
    "            indent = \"  \" * depth\n",
    "            if \"class\" in node and \"children\" not in node:\n",
    "                print(f\"{indent}{prefix}Class: {node['class']} (samples: {node['samples']})\")\n",
    "                return\n",
    "\n",
    "            feature_name = (\n",
    "                self.feature_names[node[\"feature\"]]\n",
    "                if self.feature_names\n",
    "                else node[\"feature\"]\n",
    "            )\n",
    "            print(f\"{indent}{prefix}Feature {feature_name} (samples: {node['samples']})\")\n",
    "\n",
    "            if max_depth is not None and depth >= max_depth:\n",
    "                print(f\"{indent}  ... (depth limit reached)\")\n",
    "                return\n",
    "\n",
    "            for value, child in sorted(node[\"children\"].items(), key=lambda x: str(x[0])):\n",
    "                print_recursive(child, depth + 1, f\"{value} => \")\n",
    "\n",
    "        print_recursive(self.tree)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"Save the model to a file.\n",
    "\n",
    "        Args:\n",
    "            path (str or Path): Path to save the model.\n",
    "        \"\"\"\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(path):\n",
    "        \"\"\"Load a model from a file.\n",
    "\n",
    "        Args:\n",
    "            path (str or Path): Path to the model file.\n",
    "\n",
    "        Returns:\n",
    "            ID3DecisionTree: Loaded model instance.\n",
    "        \"\"\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "\n",
    "def train_and_evaluate_id3(use_pruning=False):\n",
    "    \"\"\"Train and evaluate the ID3 decision tree model.\n",
    "\n",
    "    Args:\n",
    "        use_pruning (bool): Whether to apply post-pruning. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing model, accuracies, and tree statistics.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If required data files are missing.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Starting ID3 Decision Tree Training\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        preprocessor = UnifiedDataPreprocessor()\n",
    "        train_features, val_features, test_features, train_labels, val_labels, test_labels = (\n",
    "            preprocessor.get_processed_data(\n",
    "                discretize=True, n_bins=10, validation_split=0.2, random_state=42\n",
    "            )\n",
    "        )\n",
    "        feature_names = preprocessor.get_feature_names()\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Ensure the 'data' directory exists with adult.data and adult.test files\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nTraining ID3 model (max_depth=10, min_samples_split=20)...\")\n",
    "    id3_model = ID3DecisionTree(max_depth=10, min_samples_split=20)\n",
    "    id3_model.feature_names = [f\"{i}: {name}\" for i, name in enumerate(feature_names)]\n",
    "    id3_model.fit(train_features, train_labels)\n",
    "\n",
    "    num_nodes = id3_model.count_nodes()\n",
    "    tree_depth = id3_model.get_depth()\n",
    "    print(f\"\\nTree Statistics:\")\n",
    "    print(f\"  Number of nodes: {num_nodes}\")\n",
    "    print(f\"  Tree depth: {tree_depth}\")\n",
    "\n",
    "    train_pred = id3_model.predict(train_features)\n",
    "    val_pred = id3_model.predict(val_features)\n",
    "    train_acc_before = accuracy_score(train_labels, train_pred)\n",
    "    val_acc_before = accuracy_score(val_labels, val_pred)\n",
    "\n",
    "    print(f\"\\nBefore Pruning:\")\n",
    "    print(f\"  Training accuracy: {train_acc_before:.4f}\")\n",
    "    print(f\"  Validation accuracy: {val_acc_before:.4f}\")\n",
    "\n",
    "    if use_pruning:\n",
    "        id3_model.prune(val_features, val_labels)\n",
    "        num_nodes_after = id3_model.count_nodes()\n",
    "        tree_depth_after = id3_model.get_depth()\n",
    "        print(f\"\\nTree Statistics After Pruning:\")\n",
    "        print(f\"  Number of nodes: {num_nodes_after} (reduced by {num_nodes - num_nodes_after})\")\n",
    "        print(f\"  Tree depth: {tree_depth_after}\")\n",
    "\n",
    "        train_pred = id3_model.predict(train_features)\n",
    "        val_pred = id3_model.predict(val_features)\n",
    "        train_acc_after = accuracy_score(train_labels, train_pred)\n",
    "        val_acc_after = accuracy_score(val_labels, val_pred)\n",
    "\n",
    "        print(f\"\\nAfter Pruning:\")\n",
    "        print(f\"  Training accuracy: {train_acc_after:.4f} ({train_acc_after - train_acc_before:+.4f})\")\n",
    "        print(f\"  Validation accuracy: {val_acc_after:.4f} ({val_acc_after - val_acc_before:+.4f})\")\n",
    "    else:\n",
    "        train_acc_after, val_acc_after = train_acc_before, val_acc_before\n",
    "        num_nodes_after, tree_depth_after = num_nodes, tree_depth\n",
    "\n",
    "    if test_features is not None and test_labels is not None:\n",
    "        test_pred = id3_model.predict(test_features)\n",
    "        test_acc = accuracy_score(test_labels, test_pred)\n",
    "\n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"Test Set Results:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "        print(\"\\nClassification Report (Test Set):\")\n",
    "        print(classification_report(test_labels, test_pred, target_names=[\"<=50K\", \">50K\"]))\n",
    "\n",
    "        print(\"\\nConfusion Matrix (Test Set):\")\n",
    "        cm = confusion_matrix(test_labels, test_pred)\n",
    "        print(cm)\n",
    "        print(f\"  True Negatives:  {cm[0,0]}\")\n",
    "        print(f\"  False Positives: {cm[0,1]}\")\n",
    "        print(f\"  False Negatives: {cm[1,0]}\")\n",
    "        print(f\"  True Positives:  {cm[1,1]}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Decision Tree Structure Preview (Top 3 Levels):\")\n",
    "    print(\"=\" * 60)\n",
    "    id3_model.print_tree(max_depth=3)\n",
    "\n",
    "    model_path = Path.cwd() / \"id3_model.pkl\"\n",
    "    id3_model.save_model(model_path)\n",
    "\n",
    "    return {\n",
    "        \"model\": id3_model,\n",
    "        \"train_acc\": train_acc_after,\n",
    "        \"val_acc\": val_acc_after,\n",
    "        \"test_acc\": test_acc if test_features is not None else None,\n",
    "        \"feature_names\": feature_names,\n",
    "        \"num_nodes\": num_nodes_after,\n",
    "        \"tree_depth\": tree_depth_after,\n",
    "    }\n",
    "\n",
    "\n",
    "def save_test_predictions(actual_labels, predicted_labels, algorithm_name):\n",
    "    \"\"\"Save test set predictions to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        actual_labels (np.ndarray): Actual class labels.\n",
    "        predicted_labels (np.ndarray): Predicted class labels.\n",
    "        algorithm_name (str): Name of the algorithm used.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    Path(\"../results\").mkdir(exist_ok=True)\n",
    "    pd.DataFrame({\"actual\": actual_labels, \"predicted\": predicted_labels}).to_csv(\n",
    "        f\"../results/{algorithm_name.lower()}_predictions.csv\", index=False\n",
    "    )\n",
    "    print(f\"✓ {algorithm_name} predictions saved\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the ID3 decision tree training and evaluation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ID3 Decision Tree Implementation\")\n",
    "    print(\"Includes: Discretization, Post-Pruning, Test Set Evaluation\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results = train_and_evaluate_id3(use_pruning=False)\n",
    "\n",
    "    if results is not None:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Training Complete! Final Results Summary\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Training accuracy: {results['train_acc']:.4f}\")\n",
    "        print(f\"Validation accuracy: {results['val_acc']:.4f}\")\n",
    "        if results[\"test_acc\"] is not None:\n",
    "            print(f\"Test accuracy: {results['test_acc']:.4f}\")\n",
    "        print(f\"Decision tree nodes: {results['num_nodes']}\")\n",
    "        print(f\"Decision tree depth: {results['tree_depth']}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"To compare with no pruning, run:\")\n",
    "        print(\"train_and_evaluate_id3(use_pruning=False)\")\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(\"\\nTraining failed. Please check data files and paths\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
