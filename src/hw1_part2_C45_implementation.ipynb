{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d1fa58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "C4.5 Decision Tree Training Started\n",
      "============================================================\n",
      "============================================================\n",
      "Loading raw data\n",
      "============================================================\n",
      "Training data shape: (32561, 15)\n",
      "Test data shape: (16281, 15)\n",
      "\n",
      "Cleaning data...\n",
      "Training data: 32561 ‚Üí 30162 (removed 2399 rows)\n",
      "Test data: 16281 ‚Üí 15060 (removed 1221 rows)\n",
      "\n",
      "Encoding categorical features...\n",
      "‚úì Encoding complete for 8 categorical features\n",
      "\n",
      "============================================================\n",
      "Data preparation complete\n",
      "============================================================\n",
      "Number of features: 14\n",
      "  - Continuous features: 6\n",
      "  - Categorical features: 8\n",
      "Discretized: False\n",
      "\n",
      "Data split:\n",
      "  Training set: (27145, 14)\n",
      "  Validation set: (3017, 14)\n",
      "  Test set: (15060, 14)\n",
      "\n",
      "Label distribution (>50K proportion):\n",
      "  Training set: 24.89%\n",
      "  Validation set: 24.89%\n",
      "  Test set: 24.57%\n",
      "============================================================\n",
      "\n",
      "‚úì Attributes set up: 14 features\n",
      "‚è≥ Preprocessing continuous attributes...\n",
      "‚è≥ Building decision tree structure...\n",
      "‚è≥ Performing post-pruning...\n",
      "‚úì Pruning completed: 1437 ‚Üí 246 nodes\n",
      "\n",
      "------------------------------------------------------------\n",
      "Decision Tree Statistics:\n",
      "  Total nodes: 246\n",
      "  Leaf nodes: 207\n",
      "  Internal nodes: 39\n",
      "  Tree depth: 14\n",
      "  Average leaf samples: 131.1\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚úì Training completed, total time: 11.92 seconds\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Training Evaluation\n",
      "============================================================\n",
      "Training accuracy: 0.8439 (84.39%)\n",
      "Validation accuracy: 0.8432 (84.32%)\n",
      "\n",
      "============================================================\n",
      "C4.5 Prediction Started\n",
      "============================================================\n",
      "‚úì Using loaded test data: 15060 samples\n",
      "‚úì Prediction completed, time: 0.02 seconds\n",
      "  Average prediction speed: 775531 samples/second\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Test accuracy: 0.8398 (83.98%)\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted\n",
      "              <=50K  >50K\n",
      "Actual <=50K     10881     479\n",
      "       >50K        1934    1766\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.85      0.96      0.90     11360\n",
      "        >50K       0.79      0.48      0.59      3700\n",
      "\n",
      "    accuracy                           0.84     15060\n",
      "   macro avg       0.82      0.72      0.75     15060\n",
      "weighted avg       0.83      0.84      0.82     15060\n",
      "\n",
      "Prediction Distribution:\n",
      "  <=50K:  12815 (85.09%)\n",
      "  >50K:   2245 (14.91%)\n",
      "============================================================\n",
      "Execution Completed\n",
      "üéØ\n",
      "\n",
      "üéØ Final accuracy: 0.8398 (83.98%)\n",
      "‚è±Ô∏è  Total execution time: 11.94 seconds\n",
      "   - Training time: 11.92 seconds\n",
      "   - Prediction time: 0.02 seconds\n",
      "\n",
      "‚úÖ C4.5 Decision Tree executed successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "C4.5 Decision Tree Implementation - Refactored Version\n",
    "Handles node counting using @property for real-time computation.\n",
    "\"\"\"\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from hw1_part2_preprocessing import UnifiedDataPreprocessor\n",
    "\n",
    "class Node:\n",
    "    \"\"\"Represents a node in the decision tree.\"\"\"\n",
    "    def __init__(self, is_leaf=False, label=None, threshold=None, attribute=None, attribute_idx=None):\n",
    "        \"\"\"\n",
    "        Initialize a decision tree node.\n",
    "        Args:\n",
    "            is_leaf (bool): Whether the node is a leaf.\n",
    "            label (int): Class label for leaf nodes or attribute name for internal nodes.\n",
    "            threshold (float): Splitting threshold for continuous attributes.\n",
    "            attribute (str): Name of the splitting attribute.\n",
    "            attribute_idx (int): Index of the splitting attribute.\n",
    "        \"\"\"\n",
    "        self.is_leaf = is_leaf\n",
    "        self.label = label\n",
    "        self.attribute = attribute\n",
    "        self.attribute_idx = attribute_idx\n",
    "        self.threshold = threshold\n",
    "        self.children = {}\n",
    "        self.samples = 0\n",
    "        self.class_distribution = {}\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return a string representation of the node.\n",
    "        Returns:\n",
    "            str: String describing the node.\n",
    "        \"\"\"\n",
    "        if self.is_leaf:\n",
    "            return f\"Leaf({self.label}, samples={self.samples})\"\n",
    "        return f\"Node({self.attribute}, threshold={self.threshold}, samples={self.samples})\"\n",
    "\n",
    "class C45DecisionTree:\n",
    "    \"\"\"C4.5 Decision Tree implementation with post-pruning and real-time node counting.\"\"\"\n",
    "    DEFAULT_CLASSES = [0, 1]  # 0: <=50K, 1: >50K\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth=None,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        min_gain_ratio=0.01,\n",
    "        pruning=True,\n",
    "        validation_split=0.1,\n",
    "        data_dir=\"../data\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the C4.5 Decision Tree.\n",
    "        Args:\n",
    "            max_depth (int): Maximum depth of the tree.\n",
    "            min_samples_split (int): Minimum number of samples required to split an internal node.\n",
    "            min_samples_leaf (int): Minimum number of samples required at a leaf node.\n",
    "            min_gain_ratio (float): Minimum gain ratio threshold for splitting.\n",
    "            pruning (bool): Whether to enable post-pruning.\n",
    "            validation_split (float): Proportion of data to use for validation (for pruning).\n",
    "            data_dir (str): Directory path for data files.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_gain_ratio = min_gain_ratio\n",
    "        self.pruning = pruning\n",
    "        self.validation_split = validation_split\n",
    "        self.data_dir = data_dir\n",
    "        self.tree = None\n",
    "        self.is_fitted = False\n",
    "        self.classes = self.DEFAULT_CLASSES\n",
    "        self.attributes = []\n",
    "        self.attribute_types = {}\n",
    "        self.attribute_indices = {}\n",
    "        self.continuous_thresholds = {}\n",
    "        self.preprocessor = None\n",
    "        self.train_time = 0\n",
    "        self.predict_time = 0\n",
    "        self.x_train = None  # Êñ∞Â¢û\n",
    "        self.y_train = None  # Êñ∞Â¢û\n",
    "        self.x_val = None    # Êñ∞Â¢û\n",
    "        self.y_val = None    # Êñ∞Â¢û\n",
    "        self.x_test = None\n",
    "        self.y_test = None\n",
    "\n",
    "    @property\n",
    "    def n_nodes(self):\n",
    "        \"\"\"\n",
    "        Compute the total number of nodes in the tree.\n",
    "        Returns:\n",
    "            int: Total number of nodes.\n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            return 0\n",
    "        return self._count_all_nodes(self.tree)\n",
    "\n",
    "    @property\n",
    "    def n_leaves(self):\n",
    "        \"\"\"\n",
    "        Compute the number of leaf nodes in the tree.\n",
    "        Returns:\n",
    "            int: Number of leaf nodes.\n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            return 0\n",
    "        return self._count_leaves(self.tree)\n",
    "\n",
    "    def _count_all_nodes(self, node):\n",
    "        \"\"\"\n",
    "        Recursively count all nodes in the tree.\n",
    "        Args:\n",
    "            node (Node): Current node to count.\n",
    "        Returns:\n",
    "            int: Number of nodes in the subtree.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            return 0\n",
    "        if node.is_leaf:\n",
    "            return 1\n",
    "        count = 1\n",
    "        for child in node.children.values():\n",
    "            count += self._count_all_nodes(child)\n",
    "        return count\n",
    "\n",
    "    def _count_leaves(self, node):\n",
    "        \"\"\"\n",
    "        Recursively count leaf nodes in the tree.\n",
    "        Args:\n",
    "            node (Node): Current node to count.\n",
    "        Returns:\n",
    "            int: Number of leaf nodes in the subtree.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            return 0\n",
    "        if node.is_leaf:\n",
    "            return 1\n",
    "        count = 0\n",
    "        for child in node.children.values():\n",
    "            count += self._count_leaves(child)\n",
    "        return count\n",
    "\n",
    "    def _setup_attributes(self):\n",
    "        \"\"\"\n",
    "        Set up attribute types and indices based on the preprocessor.\n",
    "        \"\"\"\n",
    "        self.attributes = self.preprocessor.get_feature_names()\n",
    "        for attr in self.attributes:\n",
    "            self.attribute_types[attr] = (\n",
    "                \"continuous\" if attr in self.preprocessor.CONTINUOUS_FEATURES else \"discrete\"\n",
    "            )\n",
    "        self.attribute_indices = {attr: idx for idx, attr in enumerate(self.attributes)}\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Train the C4.5 Decision Tree.\n",
    "        Returns:\n",
    "            C45DecisionTree: Self reference.\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"C4.5 Decision Tree Training Started\")\n",
    "        print(\"=\" * 60)\n",
    "        start_time = time.time()\n",
    "        self.preprocessor = UnifiedDataPreprocessor(data_dir=self.data_dir)\n",
    "        if self.pruning and self.validation_split > 0:\n",
    "            x_train, x_val, x_test, y_train, y_val, y_test = self.preprocessor.get_processed_data(\n",
    "                discretize=False,\n",
    "                validation_split=self.validation_split,\n",
    "                random_state=42,\n",
    "                verbose=True,\n",
    "            )\n",
    "            self.x_val = x_val   # Êñ∞Â¢û\n",
    "            self.y_val = y_val   # Êñ∞Â¢û\n",
    "        else:\n",
    "            x_train, x_test, y_train, y_test = self.preprocessor.get_processed_data(\n",
    "                discretize=False,\n",
    "                validation_split=0.0,\n",
    "                verbose=True,\n",
    "            )\n",
    "            self.x_val = None    # Êñ∞Â¢û\n",
    "            self.y_val = None    # Êñ∞Â¢û\n",
    "        self.x_train = x_train   # Êñ∞Â¢û\n",
    "        self.y_train = y_train   # Êñ∞Â¢û\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self._setup_attributes()\n",
    "        print(f\"‚úì Attributes set up: {len(self.attributes)} features\")\n",
    "        print(\"‚è≥ Preprocessing continuous attributes...\")\n",
    "        self._preprocess_continuous_attributes(x_train)\n",
    "        print(\"‚è≥ Building decision tree structure...\")\n",
    "        self.tree = self._build_tree(x_train, y_train, list(range(len(self.attributes))), depth=0)\n",
    "        nodes_before_prune = self.n_nodes\n",
    "        if self.pruning and self.x_val is not None:\n",
    "            print(\"‚è≥ Performing post-pruning...\")\n",
    "            self._post_prune(self.tree, self.x_val, self.y_val)\n",
    "            print(f\"‚úì Pruning completed: {nodes_before_prune} ‚Üí {self.n_nodes} nodes\")\n",
    "        self.is_fitted = True\n",
    "        self.train_time = time.time() - start_time\n",
    "        self._print_tree_statistics()\n",
    "        print(f\"\\n‚úì Training completed, total time: {self.train_time:.2f} seconds\")\n",
    "        print(\"=\" * 60)\n",
    "        return self\n",
    "\n",
    "    def _preprocess_continuous_attributes(self, x):\n",
    "        \"\"\"\n",
    "        Preprocess continuous attributes by sorting and caching candidate thresholds.\n",
    "        Args:\n",
    "            x (np.ndarray): Feature matrix.\n",
    "        \"\"\"\n",
    "        for attr in self.attributes:\n",
    "            if self.attribute_types[attr] == \"continuous\":\n",
    "                attr_idx = self.attribute_indices[attr]\n",
    "                values = np.sort(np.unique(x[:, attr_idx]))\n",
    "                if len(values) > 1:\n",
    "                    thresholds = [(values[i] + values[i + 1]) / 2 for i in range(min(len(values) - 1, 100))]\n",
    "                    self.continuous_thresholds[attr] = thresholds\n",
    "                else:\n",
    "                    self.continuous_thresholds[attr] = []\n",
    "\n",
    "    def _build_tree(self, x, y, available_attrs, depth):\n",
    "        \"\"\"\n",
    "        Recursively build the decision tree using C4.5 algorithm.\n",
    "        Args:\n",
    "            x (np.ndarray): Feature matrix.\n",
    "            y (np.ndarray): Target labels.\n",
    "            available_attrs (list): Indices of available attributes for splitting.\n",
    "            depth (int): Current depth of the tree.\n",
    "        Returns:\n",
    "            Node: Root node of the subtree.\n",
    "        \"\"\"\n",
    "        n_samples = len(x)\n",
    "        class_counts = Counter(y)\n",
    "        majority_class = max(class_counts, key=class_counts.get)\n",
    "        if n_samples < self.min_samples_split or len(class_counts) == 1:\n",
    "            return self._create_leaf(majority_class, class_counts, n_samples)\n",
    "        if not available_attrs or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            return self._create_leaf(majority_class, class_counts, n_samples)\n",
    "        best_attr_idx, best_threshold, best_subsets = self._find_best_split(x, y, available_attrs)\n",
    "        if best_attr_idx is None:\n",
    "            return self._create_leaf(majority_class, class_counts, n_samples)\n",
    "        best_attr = self.attributes[best_attr_idx]\n",
    "        node = Node(\n",
    "            is_leaf=False,\n",
    "            attribute=best_attr,\n",
    "            attribute_idx=best_attr_idx,\n",
    "            threshold=best_threshold,\n",
    "        )\n",
    "        node.samples = n_samples\n",
    "        node.class_distribution = class_counts\n",
    "        remaining_attrs = [a for a in available_attrs if a != best_attr_idx]\n",
    "        for branch_value, (x_subset, y_subset) in best_subsets.items():\n",
    "            if len(x_subset) >= self.min_samples_leaf:\n",
    "                child = self._build_tree(x_subset, y_subset, remaining_attrs, depth + 1)\n",
    "                node.children[branch_value] = child\n",
    "            else:\n",
    "                subset_counts = Counter(y_subset)\n",
    "                subset_majority = max(subset_counts, key=subset_counts.get)\n",
    "                node.children[branch_value] = self._create_leaf(subset_majority, subset_counts, len(x_subset))\n",
    "        if not node.children:\n",
    "            return self._create_leaf(majority_class, class_counts, n_samples)\n",
    "        return node\n",
    "\n",
    "    def _create_leaf(self, label, class_counts, n_samples):\n",
    "        \"\"\"\n",
    "        Create a leaf node.\n",
    "        Args:\n",
    "            label (int): Class label for the leaf.\n",
    "            class_counts (Counter): Distribution of classes in the node.\n",
    "            n_samples (int): Number of samples in the node.\n",
    "        Returns:\n",
    "            Node: Leaf node.\n",
    "        \"\"\"\n",
    "        leaf = Node(is_leaf=True, label=label)\n",
    "        leaf.samples = n_samples\n",
    "        leaf.class_distribution = class_counts\n",
    "        return leaf\n",
    "\n",
    "    def _find_best_split(self, x, y, available_attrs):\n",
    "        \"\"\"\n",
    "        Find the best attribute and threshold for splitting.\n",
    "        Args:\n",
    "            x (np.ndarray): Feature matrix.\n",
    "            y (np.ndarray): Target labels.\n",
    "            available_attrs (list): Indices of available attributes.\n",
    "        Returns:\n",
    "            tuple: (best_attr_idx, best_threshold, best_subsets)\n",
    "        \"\"\"\n",
    "        best_gain_ratio = self.min_gain_ratio\n",
    "        best_attr_idx = None\n",
    "        best_threshold = None\n",
    "        best_subsets = None\n",
    "        for attr_idx in available_attrs:\n",
    "            attr = self.attributes[attr_idx]\n",
    "            if self.attribute_types[attr] == \"discrete\":\n",
    "                subsets = self._split_discrete(x, y, attr_idx)\n",
    "                if len(subsets) > 1:\n",
    "                    gain_ratio = self._calculate_gain_ratio(y, subsets)\n",
    "                    if gain_ratio > best_gain_ratio:\n",
    "                        best_gain_ratio = gain_ratio\n",
    "                        best_attr_idx = attr_idx\n",
    "                        best_threshold = None\n",
    "                        best_subsets = subsets\n",
    "            else:\n",
    "                if attr in self.continuous_thresholds:\n",
    "                    for threshold in self.continuous_thresholds[attr]:\n",
    "                        subsets = self._split_continuous(x, y, attr_idx, threshold)\n",
    "                        if len(subsets) == 2:\n",
    "                            gain_ratio = self._calculate_gain_ratio(y, subsets)\n",
    "                            if gain_ratio > best_gain_ratio:\n",
    "                                best_gain_ratio = gain_ratio\n",
    "                                best_attr_idx = attr_idx\n",
    "                                best_threshold = threshold\n",
    "                                best_subsets = subsets\n",
    "        return best_attr_idx, best_threshold, best_subsets\n",
    "\n",
    "    def _split_discrete(self, x, y, attr_idx):\n",
    "        \"\"\"\n",
    "        Split data based on a discrete attribute.\n",
    "        Args:\n",
    "            x (np.ndarray): Feature matrix.\n",
    "            y (np.ndarray): Target labels.\n",
    "            attr_idx (int): Index of the attribute to split on.\n",
    "        Returns:\n",
    "            dict: Subsets of data for each attribute value.\n",
    "        \"\"\"\n",
    "        subsets = {}\n",
    "        for i in range(len(x)):\n",
    "            value = str(x[i, attr_idx])\n",
    "            if value not in subsets:\n",
    "                subsets[value] = ([], [])\n",
    "            subsets[value][0].append(x[i])\n",
    "            subsets[value][1].append(y[i])\n",
    "        return {k: (np.array(v[0]), np.array(v[1])) for k, v in subsets.items() if len(v[0]) > 0}\n",
    "\n",
    "    def _split_continuous(self, x, y, attr_idx, threshold):\n",
    "        \"\"\"\n",
    "        Split data based on a continuous attribute and threshold.\n",
    "        Args:\n",
    "            x (np.ndarray): Feature matrix.\n",
    "            y (np.ndarray): Target labels.\n",
    "            attr_idx (int): Index of the attribute to split on.\n",
    "            threshold (float): Threshold value for splitting.\n",
    "        Returns:\n",
    "            dict: Subsets of data for left and right branches.\n",
    "        \"\"\"\n",
    "        mask = x[:, attr_idx] <= threshold\n",
    "        return {\"left\": (x[mask], y[mask]), \"right\": (x[~mask], y[~mask])}\n",
    "\n",
    "    def _calculate_gain_ratio(self, y, subsets):\n",
    "        \"\"\"\n",
    "        Compute the gain ratio for a split.\n",
    "        Args:\n",
    "            y (np.ndarray): Target labels.\n",
    "            subsets (dict): Subsets of data after splitting.\n",
    "        Returns:\n",
    "            float: Gain ratio of the split.\n",
    "        \"\"\"\n",
    "        gain = self._information_gain(y, subsets)\n",
    "        split_info = self._split_information(y, subsets)\n",
    "        return 0 if split_info == 0 else gain / split_info\n",
    "\n",
    "    def _information_gain(self, y, subsets):\n",
    "        \"\"\"\n",
    "        Compute the information gain for a split.\n",
    "        Args:\n",
    "            y (np.ndarray): Target labels.\n",
    "            subsets (dict): Subsets of data after splitting.\n",
    "        Returns:\n",
    "            float: Information gain of the split.\n",
    "        \"\"\"\n",
    "        total_entropy = self._entropy(y)\n",
    "        n_total = len(y)\n",
    "        weighted_entropy = 0\n",
    "        for x_subset, y_subset in subsets.values():\n",
    "            if len(y_subset) > 0:\n",
    "                weight = len(y_subset) / n_total\n",
    "                weighted_entropy += weight * self._entropy(y_subset)\n",
    "        return total_entropy - weighted_entropy\n",
    "\n",
    "    def _split_information(self, y, subsets):\n",
    "        \"\"\"\n",
    "        Compute the split information for a split.\n",
    "        Args:\n",
    "            y (np.ndarray): Target labels.\n",
    "            subsets (dict): Subsets of data after splitting.\n",
    "        Returns:\n",
    "            float: Split information.\n",
    "        \"\"\"\n",
    "        n_total = len(y)\n",
    "        split_info = 0\n",
    "        for x_subset, y_subset in subsets.values():\n",
    "            if len(y_subset) > 0:\n",
    "                proportion = len(y_subset) / n_total\n",
    "                split_info -= proportion * math.log2(proportion)\n",
    "        return split_info\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        \"\"\"\n",
    "        Compute the entropy of a set of labels.\n",
    "        Args:\n",
    "            y (np.ndarray): Target labels.\n",
    "        Returns:\n",
    "            float: Entropy value.\n",
    "        \"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        class_counts = Counter(y)\n",
    "        entropy = 0\n",
    "        n_samples = len(y)\n",
    "        for count in class_counts.values():\n",
    "            if count > 0:\n",
    "                p = count / n_samples\n",
    "                entropy -= p * math.log2(p)\n",
    "        return entropy\n",
    "\n",
    "    def _post_prune(self, node, x_val, y_val):\n",
    "        \"\"\"\n",
    "        Perform post-pruning on the decision tree using pessimistic pruning.\n",
    "        Args:\n",
    "            node (Node): Current node to prune.\n",
    "            x_val (np.ndarray): Validation feature matrix.\n",
    "            y_val (np.ndarray): Validation target labels.\n",
    "        \"\"\"\n",
    "        if node is None or node.is_leaf:\n",
    "            return\n",
    "        for child in list(node.children.values()):\n",
    "            self._post_prune(child, x_val, y_val)\n",
    "        error_before = self._evaluate_node(node, x_val, y_val)\n",
    "        original_is_leaf = node.is_leaf\n",
    "        original_label = node.label\n",
    "        original_children = node.children.copy()\n",
    "        majority_class = max(node.class_distribution, key=node.class_distribution.get)\n",
    "        node.is_leaf = True\n",
    "        node.label = majority_class\n",
    "        node.children = {}\n",
    "        error_after = self._evaluate_node(node, x_val, y_val)\n",
    "        if error_after > error_before:\n",
    "            node.is_leaf = original_is_leaf\n",
    "            node.label = original_label\n",
    "            node.children = original_children\n",
    "\n",
    "    def _evaluate_node(self, node, x, y):\n",
    "        \"\"\"\n",
    "        Evaluate the error rate of a node on validation data.\n",
    "        Args:\n",
    "            node (Node): Node to evaluate.\n",
    "            x (np.ndarray): Feature matrix.\n",
    "            y (np.ndarray): Target labels.\n",
    "        Returns:\n",
    "            float: Error rate.\n",
    "        \"\"\"\n",
    "        if len(x) == 0:\n",
    "            return 0\n",
    "        predictions = np.array([self._predict_one(node, x_i) for x_i in x])\n",
    "        errors = np.sum(predictions != y)\n",
    "        return errors / len(y)\n",
    "\n",
    "    def predict_test(self):  # Âéü predict() ÊîπÂêçÔºåÂ∞àÈñÄÁî®ÊñºÊ∏¨Ë©¶ÈõÜ\n",
    "        \"\"\"\n",
    "        Predict using the trained decision tree on test set.\n",
    "        Returns:\n",
    "            tuple: Predicted labels and true labels.\n",
    "        Raises:\n",
    "            ValueError: If the model has not been trained.\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not trained. Call fit() first.\")\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"C4.5 Prediction Started\")\n",
    "        print(\"=\" * 60)\n",
    "        start_time = time.time()\n",
    "        x_test = self.x_test\n",
    "        y_test = self.y_test\n",
    "        print(f\"‚úì Using loaded test data: {len(x_test)} samples\")\n",
    "        predictions = np.array([self._predict_one(self.tree, x) for x in x_test])\n",
    "        self.predict_time = time.time() - start_time\n",
    "        print(f\"‚úì Prediction completed, time: {self.predict_time:.2f} seconds\")\n",
    "        print(f\"  Average prediction speed: {len(x_test)/self.predict_time:.0f} samples/second\")\n",
    "        print(\"=\" * 60)\n",
    "        return predictions, y_test\n",
    "\n",
    "    def _predict_one(self, node, x):\n",
    "        \"\"\"\n",
    "        Predict the class for a single sample.\n",
    "        Args:\n",
    "            node (Node): Current node in the tree.\n",
    "            x (np.ndarray): Feature vector.\n",
    "        Returns:\n",
    "            int: Predicted class label.\n",
    "        \"\"\"\n",
    "        if node.is_leaf:\n",
    "            return node.label\n",
    "        attr_idx = node.attribute_idx\n",
    "        attr_value = x[attr_idx]\n",
    "        if node.threshold is None:\n",
    "            branch_key = str(attr_value)\n",
    "            if branch_key in node.children:\n",
    "                return self._predict_one(node.children[branch_key], x)\n",
    "            return max(node.class_distribution, key=node.class_distribution.get)\n",
    "        branch_key = \"left\" if attr_value <= node.threshold else \"right\"\n",
    "        if branch_key in node.children:\n",
    "            return self._predict_one(node.children[branch_key], x)\n",
    "        return max(node.class_distribution, key=node.class_distribution.get)\n",
    "\n",
    "    def evaluate(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Evaluate prediction results.\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels.\n",
    "            y_pred (np.ndarray): Predicted labels.\n",
    "        Returns:\n",
    "            dict: Evaluation metrics including accuracy, confusion matrix, predictions, and true labels.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Evaluation Results\")\n",
    "        print(\"=\" * 60)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        print(f\"Test accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=self.classes)\n",
    "        print(f\"                Predicted\")\n",
    "        print(f\"              <=50K  >50K\")\n",
    "        print(f\"Actual <=50K    {cm[0,0]:6d}   {cm[0,1]:5d}\")\n",
    "        print(f\"       >50K      {cm[1,0]:6d}   {cm[1,1]:5d}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        class_names = [\"<=50K\", \">50K\"]\n",
    "        print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "        print(\"Prediction Distribution:\")\n",
    "        pred_counts = Counter(y_pred)\n",
    "        for i, cls_name in enumerate(class_names):\n",
    "            count = pred_counts.get(i, 0)\n",
    "            percentage = count / len(y_pred) * 100\n",
    "            print(f\"  {cls_name}:  {count:5d} ({percentage:5.2f}%)\")\n",
    "        print(\"=\" * 60)\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"predictions\": y_pred,\n",
    "            \"true_labels\": y_true,\n",
    "        }\n",
    "\n",
    "    def _print_tree_statistics(self):\n",
    "        \"\"\"\n",
    "        Display decision tree statistics.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print(\"Decision Tree Statistics:\")\n",
    "        print(f\"  Total nodes: {self.n_nodes}\")\n",
    "        print(f\"  Leaf nodes: {self.n_leaves}\")\n",
    "        print(f\"  Internal nodes: {self.n_nodes - self.n_leaves}\")\n",
    "        print(f\"  Tree depth: {self._calculate_depth(self.tree)}\")\n",
    "        print(f\"  Average leaf samples: {self._calculate_avg_leaf_samples(self.tree):.1f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    def _calculate_depth(self, node):\n",
    "        \"\"\"\n",
    "        Calculate the depth of the tree.\n",
    "        Args:\n",
    "            node (Node): Current node.\n",
    "        Returns:\n",
    "            int: Depth of the subtree.\n",
    "        \"\"\"\n",
    "        if node is None or node.is_leaf:\n",
    "            return 1\n",
    "        if not node.children:\n",
    "            return 1\n",
    "        return 1 + max(self._calculate_depth(child) for child in node.children.values())\n",
    "\n",
    "    def _calculate_avg_leaf_samples(self, node):\n",
    "        \"\"\"\n",
    "        Calculate the average number of samples in leaf nodes.\n",
    "        Args:\n",
    "            node (Node): Current node.\n",
    "        Returns:\n",
    "            float: Average number of samples in leaf nodes.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            return 0\n",
    "        if node.is_leaf:\n",
    "            return node.samples\n",
    "        if not node.children:\n",
    "            return node.samples\n",
    "        leaf_samples = []\n",
    "        self._collect_leaf_samples(node, leaf_samples)\n",
    "        return sum(leaf_samples) / len(leaf_samples) if leaf_samples else 0\n",
    "\n",
    "    def _collect_leaf_samples(self, node, leaf_samples):\n",
    "        \"\"\"\n",
    "        Collect the number of samples in all leaf nodes.\n",
    "        Args:\n",
    "            node (Node): Current node.\n",
    "            leaf_samples (list): List to store leaf node sample counts.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            return\n",
    "        if node.is_leaf:\n",
    "            leaf_samples.append(node.samples)\n",
    "        else:\n",
    "            for child in node.children.values():\n",
    "                self._collect_leaf_samples(child, leaf_samples)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Execute the complete C4.5 Decision Tree workflow with unified data preprocessing.\n",
    "    \"\"\"\n",
    "    data_dir = \"../data\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"‚ùå Error: Data directory {data_dir} not found\")\n",
    "        return\n",
    "    model = C45DecisionTree(\n",
    "        max_depth=15,\n",
    "        min_samples_split=50,\n",
    "        min_samples_leaf=20,\n",
    "        min_gain_ratio=0.01,\n",
    "        pruning=True,\n",
    "        validation_split=0.1,\n",
    "        data_dir=data_dir,\n",
    "    )\n",
    "    model.fit()\n",
    "\n",
    "    # Êñ∞Â¢ûÔºöË®àÁÆó Training Accuracy\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training Evaluation\")\n",
    "    print(\"=\" * 60)\n",
    "    training_predictions = np.array([model._predict_one(model.tree, x) for x in model.x_train])\n",
    "    training_accuracy = accuracy_score(model.y_train, training_predictions)\n",
    "    print(f\"Training accuracy: {training_accuracy:.4f} ({training_accuracy*100:.2f}%)\")\n",
    "\n",
    "    # Â¶ÇÊûúÊúâÈ©óË≠âÈõÜÔºåË®àÁÆó Validation Accuracy\n",
    "    if model.x_val is not None and model.y_val is not None:\n",
    "        validation_predictions = np.array([model._predict_one(model.tree, x) for x in model.x_val])\n",
    "        validation_accuracy = accuracy_score(model.y_val, validation_predictions)\n",
    "        print(f\"Validation accuracy: {validation_accuracy:.4f} ({validation_accuracy*100:.2f}%)\")\n",
    "\n",
    "    predictions, true_labels = model.predict_test()\n",
    "    results = model.evaluate(true_labels, predictions)\n",
    "    print(\"Execution Completed\\nüéØ\")\n",
    "    print(f\"\\nüéØ Final accuracy: {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)\")\n",
    "    print(f\"‚è±Ô∏è  Total execution time: {model.train_time + model.predict_time:.2f} seconds\")\n",
    "    print(f\"   - Training time: {model.train_time:.2f} seconds\")\n",
    "    print(f\"   - Prediction time: {model.predict_time:.2f} seconds\")\n",
    "    print(\"\\n‚úÖ C4.5 Decision Tree executed successfully!\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
